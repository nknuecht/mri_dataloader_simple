{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pprint\n",
    "import tqdm\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import KFold\n",
    "# from torchinfo import summary\n",
    "\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "import skimage.transform as skTrans\n",
    "from numpy import logical_and as l_and, logical_not as l_not\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproduciablity\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleToFixed(object):\n",
    "\n",
    "    def __init__(self, new_shape, interpolation=1, channels=4):\n",
    "        self.shape= new_shape\n",
    "        self.interpolation = interpolation\n",
    "        self.channels = channels\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # print('first shape', image.shape)\n",
    "        if image is not None: # (some patients don't have segmentations)\n",
    "            if self.channels == 1:\n",
    "                short_shape = (self.shape[1], self.shape[2], self.shape[3])\n",
    "                image = skTrans.resize(image, short_shape, order=self.interpolation, preserve_range=True)  #\n",
    "                image = image.reshape(self.shape)\n",
    "            else:\n",
    "                image = skTrans.resize(image, self.shape, order=self.interpolation, preserve_range=True)  #\n",
    "\n",
    "        # print('second shape', image.shape)\n",
    "        # print()\n",
    "        return image\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \"\"\"Randomly flips (horizontally as well as vertically) the given PIL.Image with a probability of 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, prob_flip=0.5):\n",
    "        self.prob_flip= prob_flip\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if random.random() < self.prob_flip:\n",
    "            flip_type = np.random.randint(0, 3) # flip across any 3D axis\n",
    "            image = np.flip(image, flip_type)\n",
    "        return image\n",
    "\n",
    "class ZeroChannel(object):\n",
    "    \"\"\"Randomly sets channel to zero the given PIL.Image with a probability of 0.25\n",
    "    \"\"\"\n",
    "    def __init__(self, prob_zero=0.25, channels=4):\n",
    "        self.prob_zero= prob_zero\n",
    "        self.channels = channels\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if np.random.random() < self.prob_zero:\n",
    "            channel_to_zero = np.random.randint(0, self.channels) # flip across any 3D axis\n",
    "            zeros = np.zeros((image.shape[1], image.shape[2], image.shape[3]))\n",
    "            image[channel_to_zero, :, :, :] = zeros\n",
    "        return image\n",
    "\n",
    "class ZeroSprinkle(object):\n",
    "    def __init__(self, prob_zero=0.25, prob_true=0.5, channels=4):\n",
    "        self.prob_zero=prob_zero\n",
    "        self.prob_true=prob_true\n",
    "        self.channels=channels\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if self.prob_true:\n",
    "            mask = np.random.rand(image.shape[0], image.shape[1], image.shape[2], image.shape[3])\n",
    "            mask[mask < self.prob_zero] = 0\n",
    "            mask[mask > 0] = 1\n",
    "            image = image*mask\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "class MinMaxNormalize(object):\n",
    "    \"\"\"Min-Max normalization\n",
    "    \"\"\"\n",
    "    def __call__(self, image):\n",
    "        def norm(im):\n",
    "            im = im.astype(np.float32)\n",
    "            min_v = np.min(im)\n",
    "            max_v = np.max(im)\n",
    "            im = (im - min_v)/(max_v - min_v)\n",
    "            return im\n",
    "        image = norm(image)\n",
    "        return image\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, scale=1):\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if image is not None:\n",
    "            image = image.astype(np.float32)\n",
    "            image = image.reshape((image.shape[0], int(image.shape[1]/self.scale), int(image.shape[2]/self.scale), int(image.shape[3]/self.scale)))\n",
    "            image_tensor = torch.from_numpy(image)\n",
    "            return image_tensor\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for i, t in enumerate(self.transforms):\n",
    "            image = t(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bb_3D(img, pad=0):\n",
    "    '''\n",
    "    This function returns a tumor 3D bounding box using a segmentation mask\n",
    "    '''\n",
    "    xs = np.nonzero(np.sum(np.sum(img, axis=1), axis=1))\n",
    "    ys = np.nonzero(np.sum(np.sum(img, axis=0), axis=1))\n",
    "    zs = np.nonzero(np.sum(np.sum(img, axis=0), axis=0))\n",
    "    xmin, xmax = np.min(xs), np.max(xs)\n",
    "    ymin, ymax = np.min(ys), np.max(ys)\n",
    "    zmin, zmax = np.min(zs), np.max(zs)\n",
    "    bbox = (xmin-pad, ymin-pad, zmin-pad, xmax+pad, ymax+pad, zmax+pad)\n",
    "    return bbox\n",
    "\n",
    "def min_max(img):\n",
    "    '''\n",
    "    Min-max normalization\n",
    "    '''\n",
    "    return (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "def read_mri(mr_path_dict, pad=0):\n",
    "\n",
    "    image_shape = nib.load(mr_path_dict['flair']).get_fdata().shape\n",
    "    bb_seg = get_bb_3D(nib.load(mr_path_dict['flair']).get_fdata())\n",
    "    (xmin, ymin, zmin, xmax, ymax, zmax) = bb_seg\n",
    "\n",
    "    xmin = np.max([0, xmin-pad])\n",
    "    ymin = np.max([0, ymin-pad])\n",
    "    zmin = np.max([0, zmin-pad])\n",
    "\n",
    "    xmax = np.min([image_shape[0]-1, xmax+pad])\n",
    "    ymax = np.min([image_shape[1]-1, ymax+pad])\n",
    "    zmax = np.min([image_shape[2]-1, zmax+pad])\n",
    "\n",
    "\n",
    "    img_dict = {}\n",
    "    for key in ['flair', 't1', 't1ce', 't2', 'seg']:\n",
    "        img = nib.load(mr_path_dict[key])\n",
    "        img_data = img.get_fdata()\n",
    "        img_dict[key] = img_data[xmin:xmax, ymin:ymax, zmin:zmax]\n",
    "\n",
    "    stacked_img = np.stack([min_max(img_dict['flair']), min_max(img_dict['t1']),min_max(img_dict['t1ce']),min_max(img_dict['t2'])], axis=0)\n",
    "    return stacked_img, img_dict['seg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_(image, seg, predicted=False):\n",
    "    #Overlay with Predicted\n",
    "    img = image[slice, :, :, :].squeeze()\n",
    "    img = utils.make_grid(img)\n",
    "    img = img.detach().cpu().numpy()\n",
    "    \n",
    "    print(img.shape)\n",
    "    \n",
    "    # plot images\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    img_list = [img[i].T for i in range(channels)] # 1 image per channel\n",
    "    plt.imshow(np.hstack(img_list), cmap='Greys_r')\n",
    "    \n",
    "    ## plot segmentation mask ##\n",
    "    seg_img = torch.tensor(pred[slice].squeeze())\n",
    "    if not predicted:\n",
    "        seg_img = torch.tensor(seg_img.numpy()[:, ::-1].copy()) #flip\n",
    "    seg_img = utils.make_grid(seg_img).detach().cpu().numpy()\n",
    "    \n",
    "    print(np.unique(seg_img))\n",
    "\n",
    "    plt.imshow(np.hstack([seg_img[0].T]), cmap='Greys_r', alpha=0.3)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                metadata_df,\n",
    "                root_dir,\n",
    "                transform=None,\n",
    "                seg_transform=None, ###\n",
    "                dataformat=None, # indicates what shape (or content) should be returned (2D or 3D, etc.)\n",
    "                returndims=None, # what size/shape 3D volumes should be returned as.\n",
    "                output_shape=None,\n",
    "                visualize=False,\n",
    "                modality=None,\n",
    "                pad=2,\n",
    "                device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metadata_df (string): Path to the csv file w/ patient IDs\n",
    "            root_dir (string): Directory for MR images\n",
    "            transform (callable, optional)\n",
    "        \"\"\"\n",
    "        self.device=device\n",
    "        self.metadata_df = metadata_df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.seg_transform = seg_transform\n",
    "        self.returndims=returndims\n",
    "        self.modality = modality\n",
    "        self.pad = pad\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(type(idx), idx)\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        BraTS20ID = self.metadata_df.iloc[idx].BraTS_2020_subject_ID\n",
    "\n",
    "        # make dictonary of paths to MRI volumnes (modalities) and segmenation masks\n",
    "        mr_path_dict = {}\n",
    "        sequence_type = ['seg', 't1', 't1ce', 'flair', 't2']\n",
    "        for seq in sequence_type:\n",
    "            mr_path_dict[seq] = os.path.join(self.root_dir, BraTS20ID, BraTS20ID + '_'+seq+'.nii.gz')\n",
    "\n",
    "        image, seg_image = read_mri(mr_path_dict=mr_path_dict, pad=self.pad)\n",
    "        \n",
    "        if seg_image is not None:\n",
    "            if self.output_shape == 'wt':\n",
    "                seg_image[np.nonzero(seg_image)] = 1 #only 0's and 1's for background and tumor\n",
    "            else:\n",
    "                seg_image[seg_image == 4] = 3 #0,1,2,3 for background and tumor regions\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.seg_transform:\n",
    "            seg_image = self.seg_transform(seg_image)\n",
    "        else:\n",
    "            print('no transform')\n",
    "        # print(image.shape)\n",
    "        return (image, seg_image), BraTS20ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(params):\n",
    "    if platform.system() == 'Windows':\n",
    "        naming = pd.read_csv(f\"{params['image_dir']}\\\\name_mapping.csv\")\n",
    "    else:\n",
    "        naming = pd.read_csv(f\"{params['image_dir']}/name_mapping.csv\")\n",
    "    \n",
    "    data_df = pd.DataFrame(naming['BraTS_2020_subject_ID'])\n",
    "\n",
    "    # n_patients_to_train_with\n",
    "    total_num_patients = len(data_df)\n",
    "\n",
    "    assert sum(params['tr_va_te_split']) == 100\n",
    "    tr_split = int((total_num_patients * params['tr_va_te_split'][0]) / 100)\n",
    "    va_split = int((total_num_patients * params['tr_va_te_split'][1]) / 100)\n",
    "    te_split = total_num_patients - (tr_split + va_split)\n",
    "\n",
    "    print(f\"Data is split into train: {tr_split}, validation: {va_split} and test: {te_split}\")\n",
    "                   \n",
    "    train_df = data_df[: tr_split]\n",
    "    valid_df = data_df[tr_split : (tr_split + va_split)]\n",
    "    test_df = data_df[(tr_split + va_split) :]\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dataset(df, train=False):\n",
    "\n",
    "    image_dir, channels, resize_shape, output_shape = params['image_dir'], \\\n",
    "                                                      params['channels'], \\\n",
    "                                                      params['resize_shape'], \\\n",
    "                                                      params['output_shape']\n",
    "\n",
    "    # basic data augmentation\n",
    "    prob_voxel_zero = 0 # 0.1\n",
    "    prob_channel_zero = 0 # 0.5\n",
    "    prob_true = 0 # 0.8\n",
    "    randomflip = RandomFlip()\n",
    "\n",
    "    # MRI transformations\n",
    "    train_transformations = Compose([\n",
    "        MinMaxNormalize(),\n",
    "        ScaleToFixed((channels, resize_shape[0], resize_shape[1],\n",
    "                      resize_shape[2]), interpolation=1, channels=channels),\n",
    "        ZeroSprinkle(prob_zero=prob_voxel_zero, prob_true=prob_true),\n",
    "        ZeroChannel(prob_zero=prob_channel_zero),\n",
    "        randomflip,\n",
    "        ToTensor()\n",
    "        ])\n",
    "    \n",
    "    val_transformations = Compose([\n",
    "            MinMaxNormalize(),\n",
    "            ScaleToFixed((channels, resize_shape[0], resize_shape[1],\n",
    "                          resize_shape[2]), interpolation=1, channels=channels),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "\n",
    "    # GT segmentation mask transformations\n",
    "    seg_transformations = Compose([\n",
    "        ScaleToFixed((1, resize_shape[0], resize_shape[1],\n",
    "                      resize_shape[2]), interpolation=0, channels=1),\n",
    "        randomflip,\n",
    "        ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    if train:\n",
    "        dataset = GeneralDataset(metadata_df=df, \n",
    "                                root_dir=image_dir,\n",
    "                                transform=train_transformations,\n",
    "                                seg_transform=seg_transformations,\n",
    "                                returndims=resize_shape,\n",
    "                                output_shape=output_shape)\n",
    "    else:\n",
    "        dataset = GeneralDataset(metadata_df=df, \n",
    "                                root_dir=image_dir,\n",
    "                                transform=val_transformations,\n",
    "                                seg_transform=seg_transformations,\n",
    "                                returndims=resize_shape,\n",
    "                                output_shape=output_shape)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(params):\n",
    "\n",
    "    train_df, valid_df, test_df = read_dataframe(params)\n",
    "\n",
    "    train_dataset = retrieve_dataset(train_df, train=True)\n",
    "    \n",
    "    valid_dataset = retrieve_dataset(valid_df)\n",
    "\n",
    "    test_dataset = retrieve_dataset(test_df)\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['train_batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=params['train_batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['test_batch_size'])\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, norm='b', num_groups=2, k_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv3d = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=k_size,\n",
    "                                stride=stride, padding=padding)\n",
    "        if norm == 'b':\n",
    "            self.norm = nn.BatchNorm3d(num_features=out_channels)\n",
    "        else:\n",
    "            # use only one group if the given number of groups is greater than the number of channels\n",
    "            if out_channels < num_groups:\n",
    "                num_groups = 1\n",
    "            assert out_channels % num_groups == 0, f'Expected out_channels{out_channels} in input to be divisible by num_groups{num_groups}'\n",
    "            self.norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(self.conv3d(x))\n",
    "        x = F.elu(x) #!\n",
    "        return x\n",
    "\n",
    "class ConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k_size=3, stride=2, padding=1, output_padding=1):\n",
    "        super(ConvTranspose, self).__init__()\n",
    "        self.conv3d_transpose = nn.ConvTranspose3d(in_channels=in_channels,\n",
    "                                                   out_channels=out_channels,\n",
    "                                                   kernel_size=k_size,\n",
    "                                                   stride=stride,\n",
    "                                                   padding=padding,\n",
    "                                                   output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv3d_transpose(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, init_features, norm='b', num_groups=2, model_depth=4, pool_size=2):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.root_feat_maps = init_features\n",
    "        self.num_conv_blocks = 2\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "        for depth in range(model_depth):\n",
    "            feat_map_channels = 2 ** (depth + 1) * self.root_feat_maps\n",
    "            for i in range(self.num_conv_blocks):\n",
    "                self.conv_block = ConvBlock(in_channels=in_channels, out_channels=feat_map_channels, norm=norm, num_groups=num_groups)\n",
    "                self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv_block\n",
    "                in_channels, feat_map_channels = feat_map_channels, feat_map_channels * 2\n",
    "            if depth == model_depth - 1:\n",
    "                break\n",
    "            else:\n",
    "                self.pooling = nn.MaxPool3d(kernel_size=pool_size, stride=2, padding=0)\n",
    "                self.module_dict[\"max_pooling_{}\".format(depth)] = self.pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_sampling_features = []\n",
    "        for k, op in self.module_dict.items():\n",
    "            if k.startswith(\"conv\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "                if k.endswith(\"1\"):\n",
    "                    down_sampling_features.append(x)\n",
    "            elif k.startswith(\"max_pooling\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "\n",
    "        return x, down_sampling_features\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels, init_features, norm, num_groups=2, model_depth=4):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.num_conv_blocks = 2\n",
    "        self.num_feat_maps = init_features\n",
    "        # user nn.ModuleDict() to store ops\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "\n",
    "        for depth in range(model_depth - 2, -1, -1):\n",
    "            # print(depth)\n",
    "            feat_map_channels = 2 ** (depth + 1) * self.num_feat_maps\n",
    "            # print(feat_map_channels * 4)\n",
    "            self.deconv = ConvTranspose(in_channels=feat_map_channels * 4, out_channels=feat_map_channels * 4)\n",
    "            self.module_dict[\"deconv_{}\".format(depth)] = self.deconv\n",
    "            for i in range(self.num_conv_blocks):\n",
    "                if i == 0:\n",
    "                    self.conv = ConvBlock(in_channels=feat_map_channels * 6, out_channels=feat_map_channels * 2, norm=norm, num_groups=num_groups)\n",
    "                    self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv\n",
    "                else:\n",
    "                    self.conv = ConvBlock(in_channels=feat_map_channels * 2, out_channels=feat_map_channels * 2, norm=norm, num_groups=num_groups)\n",
    "                    self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv\n",
    "            if depth == 0:\n",
    "                self.final_conv = ConvBlock(in_channels=feat_map_channels * 2, out_channels=out_channels, norm=norm, num_groups=num_groups)\n",
    "                self.module_dict[\"final_conv\"] = self.final_conv\n",
    "\n",
    "    def forward(self, x, down_sampling_features):\n",
    "        \"\"\"\n",
    "        :param x: inputs\n",
    "        :param down_sampling_features: feature maps from encoder path\n",
    "        :return: output\n",
    "        \"\"\"\n",
    "        for k, op in self.module_dict.items():\n",
    "            if k.startswith(\"deconv\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "                x = torch.cat((down_sampling_features[int(k[-1])], x), dim=1)\n",
    "            elif k.startswith(\"conv\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "            else:\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, init_features, norm, num_groups=2, model_depth=4, final_activation=\"sigmoid\"):\n",
    "        super(UnetModel, self).__init__()\n",
    "        self.encoder = EncoderBlock(in_channels=in_channels,\n",
    "                                    init_features=init_features,\n",
    "                                    norm=norm, num_groups=num_groups,\n",
    "                                    model_depth=model_depth)\n",
    "        self.decoder = DecoderBlock(out_channels=out_channels,\n",
    "                                    init_features=init_features,\n",
    "                                    norm=norm, num_groups=num_groups,\n",
    "                                    model_depth=model_depth)\n",
    "        if final_activation == \"sigmoid\":\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        else:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, downsampling_features = self.encoder(x)\n",
    "        x = self.decoder(x, downsampling_features)\n",
    "        x = self.sigmoid(x)\n",
    "        # print(\"Final output shape: \", x.shape)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        # smooth factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        batch_size = targets.size(0)\n",
    "        # log_prob = torch.sigmoid(logits)\n",
    "        logits = logits.view(batch_size, -1).type(torch.FloatTensor)\n",
    "        targets = targets.view(batch_size, -1).type(torch.FloatTensor)\n",
    "        intersection = (logits * targets).sum(-1)\n",
    "        dice_score = 2. * intersection / ((logits + targets).sum(-1) + self.epsilon)\n",
    "        # dice_score = 1 - dice_score.sum() / batch_size\n",
    "        dice_score = torch.mean(1. - dice_score)\n",
    "        dice_score.requires_grad = True\n",
    "        return dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss tailored to Brats need.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, do_sigmoid=True):\n",
    "        super(EDiceLoss, self).__init__()\n",
    "        self.do_sigmoid = do_sigmoid\n",
    "        self.labels = [\"ET\", \"TC\", \"WT\"]\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "    def binary_dice(self, inputs, targets, label_index, metric_mode=False):\n",
    "        smooth = 1.\n",
    "        if self.do_sigmoid:\n",
    "            inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        if metric_mode:\n",
    "            inputs = inputs > 0.5\n",
    "            if targets.sum() == 0:\n",
    "                print(f\"No {self.labels[label_index]} for this patient\")\n",
    "                if inputs.sum() == 0:\n",
    "                    return torch.tensor(1., device=device)\n",
    "                else:\n",
    "                    return torch.tensor(0., device=device)\n",
    "            # Threshold the pred\n",
    "        intersection = EDiceLoss.compute_intersection(inputs, targets)\n",
    "        if metric_mode:\n",
    "            dice = (2 * intersection) / ((inputs.sum() + targets.sum()) * 1.0)\n",
    "        else:\n",
    "            dice = (2 * intersection + smooth) / (inputs.pow(2).sum() + targets.pow(2).sum() + smooth)\n",
    "        if metric_mode:\n",
    "            return dice\n",
    "        return 1 - dice\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_intersection(inputs, targets):\n",
    "        intersection = torch.sum(inputs * targets)\n",
    "        return intersection\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        dice = 0\n",
    "        for i in range(target.size(1)):\n",
    "            dice = dice + self.binary_dice(inputs[:, i, ...], target[:, i, ...], i)\n",
    "        final_dice = dice / target.size(1)\n",
    "        final_dice.requires_grad = True\n",
    "        return final_dice\n",
    "\n",
    "    def metric(self, inputs, target):\n",
    "        dices = []\n",
    "        for j in range(target.size(0)):\n",
    "            dice = []\n",
    "            for i in range(target.size(1)):\n",
    "                dice.append(self.binary_dice(inputs[j, i], target[j, i], i, True))\n",
    "            dices.append(dice)\n",
    "        return dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, targets, patient):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds:\n",
    "        torch tensor of size 1*C*Z*Y*X, ours BS*Z*Y*X \n",
    "    targets:\n",
    "        torch tensor of same shape\n",
    "    patient :\n",
    "        The patient ID\n",
    "    \"\"\"\n",
    "\n",
    "    assert preds.shape == targets.shape, \"Preds and targets do not have the same size\"\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    \n",
    "    preds, targets = preds.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
    "\n",
    "    metrics_list = []\n",
    "\n",
    "    metrics = dict(\n",
    "        patient_id=patient,\n",
    "    )\n",
    "    # print(targets.shape, targets.dtype, targets)\n",
    "    \n",
    "    if np.sum(targets) == 0:\n",
    "        print(f\"{label} not present for {patient}\")\n",
    "    else:\n",
    "        tp = np.sum(l_and(preds, targets))\n",
    "        tn = np.sum(l_and(l_not(preds), l_not(targets)))\n",
    "        fp = np.sum(l_and(preds, l_not(targets)))\n",
    "        fn = np.sum(l_and(l_not(preds), targets))\n",
    "\n",
    "        sens = tp / (tp + fn)\n",
    "        spec = tn / (tn + fp)\n",
    "        acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "        dice = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "    metrics[DICE] = dice\n",
    "    metrics[ACC] = acc\n",
    "    metrics[SENS] = sens\n",
    "    metrics[SPEC] = spec\n",
    "    # pp.pprint(metrics)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "    return acc, dice, metrics_list\n",
    "\n",
    "\n",
    "DICE = \"dice\"\n",
    "ACC = \"acc\"\n",
    "SENS = \"sens\"\n",
    "SPEC = \"spec\"\n",
    "METRICS = [DICE, ACC, SENS, SPEC]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dice(preds, targets):\n",
    "    return (2 * torch.sum(preds * targets)) / ((preds.sum() + preds.sum()) * 1.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_metric(train, label, metric_name):\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.semilogy(train, label=label)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'Model {metric_name} Plot')\n",
    "    plt.savefig(f'Model_{metric_name}_{label}_Plot.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def plot_result(kfolds, num_epochs, fold_train_history, fold_valid_history):\n",
    "    final_fold = {'train_loss':[],'valid_loss':[],'train_acc':[],'valid_acc':[]}\n",
    "\n",
    "    for epoch in range(num_epochs):                                      \n",
    "        final_fold['train_loss'].append(np.mean([fold_train_history[str(fold)]['train_loss'][epoch] for fold in range(kfolds)]))\n",
    "        final_fold['train_acc'].append(np.mean([fold_train_history[str(fold)]['train_acc'][epoch]for fold in range(kfolds)]))\n",
    "\n",
    "    plot_metric(final_fold['train_loss'], 'train', 'Loss')\n",
    "    plot_metric(final_fold['train_acc'], 'validation', 'Accuracy')\n",
    "\n",
    "    final_fold['valid_loss'].append([fold_valid_history[str(fold)]['valid_loss'] for fold in range(kfolds)])\n",
    "    final_fold['valid_acc'].append([fold_valid_history[str(fold)]['valid_acc'] for fold in range(kfolds)])\n",
    "\n",
    "    print(final_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, params):\n",
    "\n",
    "    test_model = UnetModel(params['pretrain_in_channels'],\n",
    "                           params['pretrain_out_channels'],\n",
    "                           params['init_features'], params['norm'],\n",
    "                           params['num_groups'],\n",
    "                           )\n",
    "\n",
    "    test_optimizer = torch.optim.AdamW(test_model.parameters(),\n",
    "                                       lr=params['learning_rate'],\n",
    "                                       )\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(path)\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    return test_model, test_optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_folds(model, optimizer, fold, epoch, loss):\n",
    "    # Saving the model\n",
    "    save_path = f'model-fold-{fold}.pth'\n",
    "\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': loss,\n",
    "                  }\n",
    "    torch.save(checkpoint, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_nofolds(model, optimizer, epoch, loss, params):\n",
    "    now = datetime.now()\n",
    "    # dd/mm/YY H:M:S\n",
    "    dt_string = now.strftime(\"%d_%H_%M\")\n",
    "\n",
    "    # Saving the model\n",
    "    save_path = f\"model_{params['output_shape']}_{params['run_name']}_{dt_string}.pth\"\n",
    "\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': loss,\n",
    "                  'params': params,\n",
    "                  }\n",
    "    torch.save(checkpoint, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldRunAll(criterion, dataset, params): \n",
    "                \n",
    "    k_folds, num_epochs, train_batch_size = params['k_folds'],\\\n",
    "                                            params['no_epochs'],\\\n",
    "                                            params['train_batch_size']\n",
    "    \n",
    "    use_cuda, loss_name, in_channels, out_channels = params['use_cuda'], \\\n",
    "                                                     params['loss_name'],\\\n",
    "                                                     params['in_channels'],\\\n",
    "                                                     params['out_channels'],\n",
    "\n",
    "    init_features, learning_rate, norm, num_groups = params['init_features'],\\\n",
    "                                                     params['learning_rate'],\\\n",
    "                                                     params['norm'], \\\n",
    "                                                     params['num_groups'],\n",
    "\n",
    "    loss_function = criterion\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    fold_train_history = {}\n",
    "    fold_valid_history = {}\n",
    "    fold_train_and_valid_acc = {}\n",
    "    fold_train_and_valid_loss = {}\n",
    "\n",
    "    print('--------------------------------')\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        valid_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        dataloader_train = DataLoader(dataset, batch_size=train_batch_size, sampler=train_subsampler, num_workers=0)\n",
    "        dataloader_valid = DataLoader(dataset, batch_size=train_batch_size, sampler=valid_subsampler, num_workers=0)\n",
    "\n",
    "        # Initialize optimizer and Model\n",
    "        model = UnetModel(in_channels=in_channels, out_channels=out_channels,\n",
    "                          init_features=init_features, norm=norm, num_groups=num_groups)\n",
    "        if use_cuda:\n",
    "            model = model.cuda()\n",
    "        #print(model)\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Run the training, testing and saving loop for defined number of epochs\n",
    "        start_time = time.time()\n",
    "\n",
    "        t_loss, t_acc, t_history, v_loss, v_acc, v_history = train_test_folds(model, loss_function, optimizer,\n",
    "                                                                               dataloader_train, dataloader_valid,\n",
    "                                                                               fold, num_epochs, use_cuda,\n",
    "                                                                               loss_name)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Epoch Time: {end_time - start_time}\")\n",
    "\n",
    "        #Saving loss results \n",
    "        fold_train_and_valid_loss[str(fold)] = [t_loss, v_loss]\n",
    "        fold_train_and_valid_acc[str(fold)] = [t_acc, v_acc]\n",
    "        fold_train_history[str(fold)] = t_history\n",
    "        fold_valid_history[str(fold)] = v_history\n",
    "\n",
    "        # Print accuracy\n",
    "        print(f'Accuracy for fold {fold}: {v_acc}')\n",
    "        print(f'Loss for fold {fold}: {v_loss}')\n",
    "        print('--------------------------------')  \n",
    "\n",
    "    return fold_train_history, fold_valid_history, fold_train_and_valid_loss, fold_train_and_valid_acc\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_folds(model, loss_function, optimizer, dataloader_train, dataloader_valid, fold, num_epochs, use_cuda, loss_name):\n",
    "    train_history = {'train_loss': [], 'train_acc':[], 'train_dice':[]}\n",
    "    valid_history = {'valid_loss': [], 'valid_acc':[], 'valid_dice':[]}\n",
    "    best = math.inf\n",
    "\n",
    "    edice = EDiceLoss()\n",
    "    if use_cuda:\n",
    "        edice = edice.cuda()\n",
    "    metric = edice.metric\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Starting Train epoch: {epoch+1}')\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc, train_dice = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader_train)):\n",
    "            (inputs, targets), ID = data\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape, outputs.shape, targets.squeeze(1).long().shape)\n",
    "\n",
    "            if loss_name == 'dice':\n",
    "                class_outputs = outputs.argmax(dim=1)\n",
    "                loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "            else:\n",
    "                loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "            train_loss += loss.item() * outputs.size(0) #multiplying by batchsize\n",
    "            \n",
    "            rtrain_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "            rtrain_acc, rtrain_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "            print(f'Train Dice 1: {rtrain_dice1}, 2: {rtrain_dice2} \\t Acc: {rtrain_acc}')\n",
    "            train_acc += rtrain_acc\n",
    "            train_dice += rtrain_dice1\n",
    "            \n",
    "            print(f'Train Loss :{loss.item()}')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "        train_history['train_loss'].append(train_loss / len(dataloader_train.sampler))\n",
    "        train_history['train_acc'].append(train_acc / len(dataloader_train.sampler))\n",
    "        train_history['train_dice'].append(train_dice / len(dataloader_train.sampler))\n",
    "\n",
    "        print(f\"Train Epoch loss: {train_history['train_loss'][-1]}, \\t ACC/DICE :{train_history['train_acc'][-1]}/{train_history['train_dice'][-1]} \")\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    valid_acc, valid_dice = 0, 0\n",
    "           \n",
    "    model.eval()\n",
    "    #! maybe change later to validate after some epochs\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader_valid)):\n",
    "            (inputs, targets), ID = data\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda() \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if params['loss_name'] == 'dice':\n",
    "                class_outputs = outputs.argmax(dim=1)\n",
    "                loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "            else:\n",
    "                loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "            # print('Valid Loss:', loss.item())\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            rvalid_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "\n",
    "            rvalid_acc, rvalid_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "            # print(f'Val Dice 1: {rvalid_dice1}, 2: {rvalid_dice2}')\n",
    "            valid_acc += rvalid_acc\n",
    "            valid_dice += rvalid_dice1\n",
    "            \n",
    "        # Print accuracy\n",
    "        print(f'Val Dice : {valid_dice}, len {len(dataloader_valid.sampler)}')\n",
    "        valid_loss /= len(dataloader_valid.sampler) \n",
    "        valid_acc = valid_acc / len(dataloader_valid.sampler)\n",
    "        valid_dice = valid_dice / len(dataloader_valid.sampler)\n",
    "        #print(f\" Fold Accuracy: {valid_acc}\")\n",
    "\n",
    "    valid_history['valid_loss'].append(valid_loss)\n",
    "    valid_history['valid_acc'].append(valid_acc)\n",
    "    valid_history['valid_dice'].append(valid_dice)\n",
    "\n",
    "    print(f\"Val Epoch loss: {valid_history['valid_loss'][-1]} \\t acc/dice:/ {valid_history['valid_acc'][-1]}/ {valid_history['valid_dice'][-1]}\")\n",
    "\n",
    "    # saving best model for this fold\n",
    "    if valid_loss < best:\n",
    "        best = valid_loss\n",
    "        save_model_folds(model, optimizer, fold, epoch, loss)\n",
    "    \n",
    "    \n",
    "    return train_history['train_loss'][-1], train_history['train_acc'][-1], train_history, valid_history['valid_loss'][-1], valid_history['valid_acc'][-1], valid_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_nofolds(model, loss_function, optimizer, dataloader_train, dataloader_valid, params):\n",
    "    train_history = {'train_loss': [], 'train_acc':[], 'train_dice':[]}\n",
    "    valid_history = {'valid_loss': [], 'valid_acc':[], 'valid_dice':[]}\n",
    "    best = math.inf\n",
    "\n",
    "    edice = EDiceLoss()\n",
    "    if params['use_cuda']:\n",
    "        edice = edice.cuda()\n",
    "    metric = edice.metric\n",
    "\n",
    "    #Automatic mixed precision addition\n",
    "    #scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "    for epoch in range(params['no_epochs']):\n",
    "        print(f'Starting Train epoch: {epoch+1}')\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc, train_dice = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader_train)):\n",
    "            (inputs, targets), ID = data\n",
    "            if params['use_cuda']:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #with torch.cuda.amp.autocast(enabled=params['autocast']):\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape, outputs.shape, targets.squeeze(1).long().shape)\n",
    "\n",
    "            if params['loss_name'] == 'dice':\n",
    "                class_outputs = outputs.argmax(dim=1)\n",
    "                loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "            else:\n",
    "                loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "            train_loss += loss.item() * outputs.size(0) # multiplying by batchsize\n",
    "            \n",
    "            rtrain_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "            rtrain_acc, rtrain_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Train Dice 1: {rtrain_dice1}, 2: {rtrain_dice2} \\t Acc: {rtrain_acc}')\n",
    "\n",
    "            train_acc += rtrain_acc\n",
    "            train_dice += rtrain_dice1\n",
    "            if i % 10 == 0:\n",
    "                print(f'Train Loss :{loss.item()}')\n",
    "            \n",
    "            #scaler.scale(loss).backward()\n",
    "            #scaler.step(optimizer)\n",
    "            #scaler.update()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "        train_history['train_loss'].append(train_loss / len(dataloader_train.sampler))\n",
    "        train_history['train_acc'].append(train_acc / len(dataloader_train.sampler))\n",
    "        train_history['train_dice'].append(train_dice / len(dataloader_train.sampler))\n",
    "\n",
    "        print(f\"Train Epoch loss: {train_history['train_loss'][-1]}, \\t ACC/DICE :{train_history['train_acc'][-1]}/{train_history['train_dice'][-1]} \")\n",
    "          \n",
    "        if epoch % (.1 * params['no_epochs']) == 0:\n",
    "            valid_loss = 0.0\n",
    "            valid_acc, valid_dice = 0, 0\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Iterate over the test data and generate predictions\n",
    "                for i, data in enumerate(tqdm.tqdm(dataloader_valid)):\n",
    "                    (inputs, targets), ID = data\n",
    "                    if params['use_cuda']:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda() \n",
    "                      \n",
    "                    #with torch.cuda.amp.autocast(enabled=params['autocast']):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    if params['loss_name'] == 'dice':\n",
    "                        class_outputs = outputs.argmax(dim=1)\n",
    "                        loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                        # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "                    else:\n",
    "                        loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "                    # print('Valid Loss:', loss.item())\n",
    "                    valid_loss += loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    rvalid_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "\n",
    "                    rvalid_acc, rvalid_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "                    # print(f'Val Dice 1: {rvalid_dice1}, 2: {rvalid_dice2}')\n",
    "                    valid_acc += rvalid_acc\n",
    "                    valid_dice += rvalid_dice1\n",
    "                    \n",
    "                # Print accuracy\n",
    "                print(f'Val Dice : {valid_dice}, len {len(dataloader_valid.sampler)}')\n",
    "                valid_loss /= len(dataloader_valid.sampler) \n",
    "                valid_acc = valid_acc / len(dataloader_valid.sampler)\n",
    "                valid_dice = valid_dice / len(dataloader_valid.sampler)\n",
    "\n",
    "            valid_history['valid_loss'].append(valid_loss)\n",
    "            valid_history['valid_acc'].append(valid_acc)\n",
    "            valid_history['valid_dice'].append(valid_dice)\n",
    "\n",
    "            print(f\"Val Epoch loss: {valid_history['valid_loss'][-1]} \\t acc/dice:/ {valid_history['valid_acc'][-1]}/ {valid_history['valid_dice'][-1]}\")\n",
    "\n",
    "            # saving best model for this fold\n",
    "            if valid_loss < best:\n",
    "                best = valid_loss\n",
    "                save_model_nofolds(model, optimizer, epoch, loss, params)\n",
    "    \n",
    "    \n",
    "    return train_history['train_loss'][-1], train_history['train_acc'][-1], train_history, valid_history['valid_loss'][-1], valid_history['valid_acc'][-1], valid_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfolds(params):\n",
    "    \n",
    "    if params['loss_name'] == 'ce':\n",
    "        criterion = CrossEntropyLoss()\n",
    "    elif params['loss_name'] == 'wce':\n",
    "        wisdom_weights = [1, 355.36116969, 74.37872817, 254.58104099]\n",
    "        nick_weights = [ 1.        ,  8.9263424 ,  7.79622053, 31.17438108]\n",
    "        criterion = CrossEntropyLoss(weight=torch.Tensor(nick_weights))\n",
    "    else:\n",
    "        criterion = EDiceLoss().cuda()\n",
    "      \n",
    "    if params['use_cuda']:\n",
    "        criterion.cuda()\n",
    "    \n",
    "    train_df, valid_df, _ = read_dataframe(params)\n",
    "\n",
    "    train_valid_df = pd.concat([train_df, valid_df])\n",
    "\n",
    "    train_valid_dataset = retrieve_dataset(train_valid_df)\n",
    "\n",
    "    t_history, v_history, tv_loss, tv_acc = kFoldRunAll(criterion, \n",
    "                                                        train_valid_dataset,\n",
    "                                                        params)\n",
    "    \n",
    "    plot_result(k_folds, no_epochs, t_history, v_history)\n",
    "\n",
    "    return t_history, v_history, tv_loss, tv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(path, params):\n",
    "    \n",
    "    if params['loss_name'] == 'ce':\n",
    "        criterion = CrossEntropyLoss()\n",
    "    elif params['loss_name'] == 'wce':\n",
    "        criterion = CrossEntropyLoss(weight=torch.Tensor([1, 355.36116969, 74.37872817, 254.58104099]))\n",
    "    else:\n",
    "        criterion = EDiceLoss().cuda()\n",
    "    \n",
    "    model, optimizer, _, __ = load_checkpoint(path, params)\n",
    "\n",
    "    # Transfer by changing(replacing) only last layer and finetuning to outdim=4\n",
    "    model.decoder.final_conv = ConvBlock(in_channels=params['pretrain_in_final_conv'], \n",
    "                                          out_channels=params['out_channels'],\n",
    "                                          norm=params['norm'],\n",
    "                                          num_groups=params['num_groups'])\n",
    "\n",
    "    model.decoder.module_dict.final_conv = ConvBlock(in_channels=params['pretrain_in_final_conv'], \n",
    "                                          out_channels=params['out_channels'],\n",
    "                                          norm=params['norm'],\n",
    "                                          num_groups=params['num_groups'])\n",
    "\n",
    "    if params['use_cuda']:\n",
    "        model.cuda()\n",
    "        criterion.cuda()\n",
    "    \n",
    "    dataloader_train, dataloader_valid, _ = get_data(params)\n",
    "    \n",
    "    t_loss, t_acc, t_history, v_loss, v_acc, v_history = train_test_nofolds(\n",
    "                                                model, criterion, optimizer,\n",
    "                                                dataloader_train, dataloader_valid,\n",
    "                                                params\n",
    "                                                )\n",
    "\n",
    "    # plot_result(k_folds, params['no_epochs'], t_history, v_history)\n",
    "\n",
    "    return t_loss, t_acc, t_history, v_loss, v_acc, v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_from_scratch(params):\n",
    "    \n",
    "    if params['loss_name'] == 'ce':\n",
    "        criterion = CrossEntropyLoss()\n",
    "    elif params['loss_name'] == 'wce':\n",
    "        criterion = CrossEntropyLoss(weight=torch.Tensor([1, 355.36116969, 74.37872817, 254.58104099]))\n",
    "    else:\n",
    "        criterion = EDiceLoss()\n",
    "    \n",
    "\n",
    "\n",
    "    model = UnetModel(params['in_channels'],\n",
    "                      params['out_channels'],\n",
    "                      params['init_features'],\n",
    "                      params['norm'],\n",
    "                      params['num_groups'],\n",
    "                      )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=params['learning_rate'],\n",
    "                                  )\n",
    "\n",
    "    if params['use_cuda']:\n",
    "        model.cuda()\n",
    "        criterion.cuda()\n",
    "    \n",
    "    dataloader_train, dataloader_valid, _ = get_data(params)\n",
    "    \n",
    "    t_loss, t_acc, t_history, v_loss, v_acc, v_history = train_test_nofolds(\n",
    "                                                model, criterion, optimizer,\n",
    "                                                dataloader_train, dataloader_valid,\n",
    "                                                params\n",
    "                                                )\n",
    "\n",
    "    # plot_result(k_folds, params['no_epochs'], t_history, v_history)\n",
    "\n",
    "    return t_loss, t_acc, t_history, v_loss, v_acc, v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "\n",
    "    if platform.system() == 'Windows':\n",
    "        image_dir = r\"C:\\Users\\wisdomik\\Documents\\project\\MICCAI_BraTS2020_TrainingData\"\n",
    "    else:\n",
    "        image_dir = '../../data/data/mri/MICCAI_BraTS2020_TrainingData/'\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    params = {'run_name': 'run_1',\n",
    "              'in_channels': 4,\n",
    "              'out_channels': 4,\n",
    "              'no_epochs': 20,\n",
    "              'k_folds': 5,\n",
    "              'learning_rate': 5e-4, # 1e-4,\n",
    "              'loss_name': 'wce',\n",
    "              'output_shape': 'all',\n",
    "              'tr_va_te_split': [75, 25, 0],\n",
    "              'pretrain_in_channels': 4,\n",
    "              'pretrain_out_channels': 2,\n",
    "              'pretrain_in_final_conv': 16,\n",
    "              'init_features': 8,\n",
    "              'train_batch_size': 1,\n",
    "              'autocast': False, # not in use\n",
    "              'test_batch_size': 1,\n",
    "              'norm': 'g',\n",
    "              'num_groups': 4,\n",
    "              'channels': 4,\n",
    "              'resize_shape': (144, 144, 144), # 128\n",
    "              'image_dir': image_dir,\n",
    "              'use_cuda': use_cuda\n",
    "              }\n",
    "              \n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_name': 'run_1',\n",
       " 'in_channels': 4,\n",
       " 'out_channels': 4,\n",
       " 'no_epochs': 20,\n",
       " 'k_folds': 5,\n",
       " 'learning_rate': 0.0005,\n",
       " 'loss_name': 'wce',\n",
       " 'output_shape': 'all',\n",
       " 'tr_va_te_split': [75, 25, 0],\n",
       " 'pretrain_in_channels': 4,\n",
       " 'pretrain_out_channels': 2,\n",
       " 'pretrain_in_final_conv': 16,\n",
       " 'init_features': 8,\n",
       " 'train_batch_size': 1,\n",
       " 'autocast': False,\n",
       " 'test_batch_size': 1,\n",
       " 'norm': 'g',\n",
       " 'num_groups': 4,\n",
       " 'channels': 4,\n",
       " 'resize_shape': (144, 144, 144),\n",
       " 'image_dir': '../../data/data/mri/MICCAI_BraTS2020_TrainingData/',\n",
       " 'use_cuda': True}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set params then run this\n",
    "\n",
    "#load in trained model for evaluation\n",
    "# fold_to_check = 9\n",
    "# PATH = f'drive/MyDrive/Colab Notebooks/model-fold-{fold_to_check}.pth'\n",
    "\n",
    "params = get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/276 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is split into train: 276, validation: 92 and test: 1\n",
      "Starting Train epoch: 1\n",
      "Train Dice 1: 0.025612566620111465, 2: 0.04134624607639819 \t Acc: 0.22347474065500686\n",
      "Train Loss :1.4356637001037598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 11/276 [00:49<19:16,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.04484144598245621, 2: 0.04583972751076452 \t Acc: 0.32659250685871055\n",
      "Train Loss :1.5656408071517944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 21/276 [01:35<21:54,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2602895498275757, 2: 0.1457800531548134 \t Acc: 0.2697770651148834\n",
      "Train Loss :1.2823047637939453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 31/276 [02:17<16:19,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18175466358661652, 2: 0.12217224938362517 \t Acc: 0.3692276984739369\n",
      "Train Loss :1.2618675231933594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 41/276 [03:00<16:33,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.24503342807292938, 2: 0.17270304096122435 \t Acc: 0.38746255840620714\n",
      "Train Loss :1.26203191280365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 51/276 [03:42<15:21,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.025027401745319366, 2: 0.027662905549296242 \t Acc: 0.48958802190500683\n",
      "Train Loss :1.3376550674438477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 61/276 [04:26<14:49,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.20477481186389923, 2: 0.1346265987266496 \t Acc: 0.39267792459705075\n",
      "Train Loss :1.198861002922058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 71/276 [05:11<14:33,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05887152627110481, 2: 0.0923110919297697 \t Acc: 0.2378941079389575\n",
      "Train Loss :1.4800654649734497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 81/276 [05:52<12:50,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2516501843929291, 2: 0.17371050757934553 \t Acc: 0.5588666918509945\n",
      "Train Loss :1.296236515045166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 91/276 [06:39<15:09,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10976696014404297, 2: 0.08725294563111914 \t Acc: 0.5709243586033951\n",
      "Train Loss :1.1908483505249023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 101/276 [07:23<13:36,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08324765413999557, 2: 0.08450888983787337 \t Acc: 0.5699471263074417\n",
      "Train Loss :1.2389973402023315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 111/276 [08:05<11:03,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03660077974200249, 2: 0.03002300921416027 \t Acc: 0.5348129795739026\n",
      "Train Loss :1.3444405794143677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 121/276 [08:49<12:35,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.06960099190473557, 2: 0.1286918751426536 \t Acc: 0.5781136134687929\n",
      "Train Loss :1.3693088293075562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 131/276 [09:35<11:01,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.04361986368894577, 2: 0.04236256569219014 \t Acc: 0.44802617830504116\n",
      "Train Loss :1.340135931968689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 141/276 [10:20<09:47,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.22386065125465393, 2: 0.176724858763672 \t Acc: 0.5993719323345336\n",
      "Train Loss :1.334633469581604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 151/276 [11:03<09:54,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03826045244932175, 2: 0.04810582835226549 \t Acc: 0.5721078880529835\n",
      "Train Loss :1.3024797439575195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 161/276 [11:47<08:07,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.3015596866607666, 2: 0.22634078200147514 \t Acc: 0.6423939311128258\n",
      "Train Loss :1.1875072717666626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 171/276 [12:27<06:57,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.025298211723566055, 2: 0.024192793601452687 \t Acc: 0.55655991458762\n",
      "Train Loss :1.3353639841079712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 181/276 [13:11<06:56,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07488054782152176, 2: 0.06712615454131972 \t Acc: 0.5796246731395748\n",
      "Train Loss :1.2962720394134521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 191/276 [13:54<06:04,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05659623071551323, 2: 0.08512300218022008 \t Acc: 0.5716587898662552\n",
      "Train Loss :1.2953600883483887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 201/276 [14:40<06:06,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.14607951045036316, 2: 0.13795470144131777 \t Acc: 0.589463305898491\n",
      "Train Loss :1.3400055170059204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 211/276 [15:22<04:24,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07948766648769379, 2: 0.11207326320983713 \t Acc: 0.590217831039952\n",
      "Train Loss :1.330883502960205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 221/276 [16:05<03:42,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.22275646030902863, 2: 0.2226019650206448 \t Acc: 0.6489763508444787\n",
      "Train Loss :1.198224663734436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 231/276 [16:48<03:18,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05495570972561836, 2: 0.045874751642744 \t Acc: 0.5812140989368999\n",
      "Train Loss :1.194611668586731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 241/276 [17:28<02:28,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.008762557990849018, 2: 0.00938763014966136 \t Acc: 0.517757295417524\n",
      "Train Loss :1.3486034870147705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 251/276 [18:09<01:50,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.060977786779403687, 2: 0.0686034479159522 \t Acc: 0.581607938957476\n",
      "Train Loss :1.280258059501648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 261/276 [18:49<00:59,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10280238091945648, 2: 0.09020203181905309 \t Acc: 0.6026087212791496\n",
      "Train Loss :1.336519479751587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 271/276 [19:30<00:21,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2011374831199646, 2: 0.17527629594615887 \t Acc: 0.6318801440329218\n",
      "Train Loss :1.151358723640442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [19:52<00:00,  4.32s/it]\n",
      "  0%|          | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch loss: 1.2970670500527257, \t ACC/DICE :0.5110606354336732/0.11756820976734161 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 92/92 [06:30<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Dice : 10.687841415405273, len 92\n",
      "Val Epoch loss: 1.2212159361528314 \t acc/dice:/ 0.5991750341654177/ 0.116172194480896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/276 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Train epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/276 [00:03<15:42,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.12764644622802734, 2: 0.10902627672595641 \t Acc: 0.5875229070216049\n",
      "Train Loss :1.3156973123550415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 11/276 [00:38<15:43,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08103632926940918, 2: 0.08936985489363504 \t Acc: 0.5966441883144719\n",
      "Train Loss :1.1962621212005615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 21/276 [01:15<15:39,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.12764474749565125, 2: 0.1312088920079816 \t Acc: 0.6111161345807613\n",
      "Train Loss :1.2037479877471924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 31/276 [01:53<15:30,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03200957924127579, 2: 0.036242230105894585 \t Acc: 0.6138867455418381\n",
      "Train Loss :1.325472116470337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 41/276 [02:31<15:03,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1765252947807312, 2: 0.13508241139940663 \t Acc: 0.6048843530306928\n",
      "Train Loss :1.1847271919250488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 51/276 [03:09<14:23,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10857203602790833, 2: 0.10405215754976098 \t Acc: 0.6211446544924554\n",
      "Train Loss :1.3132230043411255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 61/276 [03:48<13:46,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10438639670610428, 2: 0.12099078953775391 \t Acc: 0.6070357376328875\n",
      "Train Loss :1.1853020191192627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 71/276 [04:26<13:07,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15807104110717773, 2: 0.2559194116352432 \t Acc: 0.6739145286779835\n",
      "Train Loss :1.1372126340866089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 81/276 [05:05<12:33,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.23200677335262299, 2: 0.19812771017939756 \t Acc: 0.6296597704475309\n",
      "Train Loss :1.305237054824829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 91/276 [05:44<11:56,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16071465611457825, 2: 0.1780377718178505 \t Acc: 0.6170552152992113\n",
      "Train Loss :1.2612611055374146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 101/276 [06:22<11:15,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05371512472629547, 2: 0.04803452435497558 \t Acc: 0.6096439230752744\n",
      "Train Loss :1.1879056692123413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 111/276 [07:01<10:38,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.045875389128923416, 2: 0.036592739312321 \t Acc: 0.578435115526406\n",
      "Train Loss :1.325801134109497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 121/276 [07:39<09:58,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.24962204694747925, 2: 0.19377168162561043 \t Acc: 0.6285073865097737\n",
      "Train Loss :1.1404602527618408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 131/276 [08:18<09:22,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.12211979925632477, 2: 0.1183220794427104 \t Acc: 0.6408982767489712\n",
      "Train Loss :1.3038662672042847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 141/276 [08:57<08:42,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.02604909986257553, 2: 0.02385678484468652 \t Acc: 0.5960095566486625\n",
      "Train Loss :1.2795342206954956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 151/276 [09:35<08:03,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.019579054787755013, 2: 0.02007707231013126 \t Acc: 0.5781169624485597\n",
      "Train Loss :1.3551989793777466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 161/276 [10:14<07:25,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16179072856903076, 2: 0.17550235934292713 \t Acc: 0.6493085696373457\n",
      "Train Loss :1.1518036127090454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 171/276 [10:52<06:44,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.13935935497283936, 2: 0.11614697567771047 \t Acc: 0.6375064300411523\n",
      "Train Loss :1.2370798587799072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 181/276 [11:31<06:07,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.009681533090770245, 2: 0.010952637028356783 \t Acc: 0.5873172796639232\n",
      "Train Loss :1.322075366973877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 191/276 [12:10<05:28,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07612819969654083, 2: 0.07360801464100014 \t Acc: 0.6628203634044925\n",
      "Train Loss :1.3070441484451294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 201/276 [12:48<04:50,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05731318145990372, 2: 0.06451498467476445 \t Acc: 0.6467469350137174\n",
      "Train Loss :1.253334641456604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 211/276 [13:27<04:12,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10656635463237762, 2: 0.07337714365245653 \t Acc: 0.5875125251843278\n",
      "Train Loss :1.1839444637298584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 221/276 [14:06<03:32,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18965715169906616, 2: 0.22170611199219226 \t Acc: 0.6581535600994513\n",
      "Train Loss :1.176035761833191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 231/276 [14:45<02:54,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16157536208629608, 2: 0.12611992784768417 \t Acc: 0.6367311412251372\n",
      "Train Loss :1.1206719875335693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 241/276 [15:23<02:15,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.14219099283218384, 2: 0.1396718925117244 \t Acc: 0.6223499523105281\n",
      "Train Loss :1.3298652172088623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 251/276 [16:02<01:36,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07858806848526001, 2: 0.10541902610721278 \t Acc: 0.6018230506258574\n",
      "Train Loss :1.3801623582839966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 261/276 [16:41<00:57,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.14131391048431396, 2: 0.13009317798590905 \t Acc: 0.6366581334662208\n",
      "Train Loss :1.2586435079574585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 271/276 [17:19<00:19,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2160724401473999, 2: 0.22593346808470446 \t Acc: 0.6569241496270576\n",
      "Train Loss :1.132326364517212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [17:39<00:00,  3.84s/it]\n",
      "  0%|          | 0/276 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch loss: 1.260661724684895, \t ACC/DICE :0.6235798796906689/0.13054293394088745 \n",
      "Starting Train epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/276 [00:03<15:01,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.28551849722862244, 2: 0.23706981921992065 \t Acc: 0.6632999373070988\n",
      "Train Loss :1.1461542844772339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 11/276 [00:41<16:58,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03422345593571663, 2: 0.04578372094416846 \t Acc: 0.5733754768947188\n",
      "Train Loss :1.2304340600967407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 21/276 [01:20<16:28,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.3699691891670227, 2: 0.25183550131022925 \t Acc: 0.6603705847050755\n",
      "Train Loss :1.2900198698043823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 31/276 [01:58<15:46,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1446310132741928, 2: 0.14837338670038244 \t Acc: 0.6106519659850823\n",
      "Train Loss :1.1792303323745728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 41/276 [02:37<15:06,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.06100082769989967, 2: 0.062360737379011204 \t Acc: 0.6090689032493142\n",
      "Train Loss :1.3305118083953857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 51/276 [03:16<14:32,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.23813626170158386, 2: 0.19264845975750294 \t Acc: 0.6372733410493827\n",
      "Train Loss :1.3225902318954468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 61/276 [03:54<13:53,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.12175139039754868, 2: 0.11611608950794398 \t Acc: 0.6213777434842249\n",
      "Train Loss :1.3393275737762451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 71/276 [04:33<13:12,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.19531121850013733, 2: 0.13590852105166884 \t Acc: 0.6756904926483196\n",
      "Train Loss :1.1292613744735718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 81/276 [05:12<12:38,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2640267312526703, 2: 0.19884437726222579 \t Acc: 0.6499746817129629\n",
      "Train Loss :1.329275131225586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 91/276 [05:51<11:57,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1431044638156891, 2: 0.17190688885071292 \t Acc: 0.6771362472136488\n",
      "Train Loss :1.35334050655365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 101/276 [06:29<11:14,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16481131315231323, 2: 0.14176394429649591 \t Acc: 0.6450232151277435\n",
      "Train Loss :1.237870216369629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 111/276 [07:08<10:41,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07078944891691208, 2: 0.07076928812169254 \t Acc: 0.6326979648919753\n",
      "Train Loss :1.2388259172439575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 121/276 [07:47<09:59,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.0422416590154171, 2: 0.037289610464545084 \t Acc: 0.5865088359482168\n",
      "Train Loss :1.2222312688827515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 131/276 [08:26<09:20,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.22610566020011902, 2: 0.20450945715941857 \t Acc: 0.6392127352323388\n",
      "Train Loss :1.310491919517517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 141/276 [09:04<08:44,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.32695063948631287, 2: 0.2658984926285559 \t Acc: 0.6507258578746571\n",
      "Train Loss :1.2864934206008911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 151/276 [09:43<08:06,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.24745990335941315, 2: 0.18063286998928837 \t Acc: 0.6459672925240055\n",
      "Train Loss :1.3480421304702759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 161/276 [10:22<07:26,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15328574180603027, 2: 0.12861606156357414 \t Acc: 0.6381507737482853\n",
      "Train Loss :1.1181141138076782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 171/276 [11:01<06:47,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08692029863595963, 2: 0.08018765124377066 \t Acc: 0.6183670106738683\n",
      "Train Loss :1.1670843362808228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 181/276 [11:40<06:09,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15469259023666382, 2: 0.13007876744723995 \t Acc: 0.6334987059542181\n",
      "Train Loss :1.3364366292953491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 191/276 [12:19<05:30,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09946002811193466, 2: 0.1090788049465023 \t Acc: 0.6262156796553497\n",
      "Train Loss :1.152700662612915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 201/276 [12:57<04:50,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18762889504432678, 2: 0.2665027116023534 \t Acc: 0.6669874989283264\n",
      "Train Loss :1.2278339862823486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 211/276 [13:36<04:11,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08106313645839691, 2: 0.07882018484946558 \t Acc: 0.6291966065457819\n",
      "Train Loss :1.2736802101135254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 221/276 [14:15<03:33,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.04751935601234436, 2: 0.04614780166566692 \t Acc: 0.6048545471107681\n",
      "Train Loss :1.2784249782562256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 231/276 [14:54<02:54,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03497398644685745, 2: 0.03814971686199989 \t Acc: 0.6023223835090878\n",
      "Train Loss :1.2882729768753052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 241/276 [15:33<02:16,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03681754320859909, 2: 0.0310271093791206 \t Acc: 0.6246955777391975\n",
      "Train Loss :1.2889667749404907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 251/276 [16:12<01:37,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1639164686203003, 2: 0.12587998871585834 \t Acc: 0.6305760513117284\n",
      "Train Loss :1.143691062927246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 261/276 [16:50<00:58,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.20893150568008423, 2: 0.14620025396288247 \t Acc: 0.641739875364369\n",
      "Train Loss :1.1364750862121582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 271/276 [17:29<00:19,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.13580353558063507, 2: 0.15616174832240848 \t Acc: 0.6509599515603567\n",
      "Train Loss :1.1915090084075928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [17:49<00:00,  3.87s/it]\n",
      "  0%|          | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch loss: 1.263738748388014, \t ACC/DICE :0.6286282798257613/0.13062070310115814 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 92/92 [05:09<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Dice : 11.47465991973877, len 92\n",
      "Val Epoch loss: 1.2110050825969032 \t acc/dice:/ 0.6425074360455525/ 0.12472456693649292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/276 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Train epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/276 [00:03<15:15,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.046421341598033905, 2: 0.0608318223322356 \t Acc: 0.6365924934627915\n",
      "Train Loss :1.3444098234176636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 11/276 [00:38<15:45,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.17537479102611542, 2: 0.21876026682529137 \t Acc: 0.6384515121313443\n",
      "Train Loss :1.1861534118652344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 21/276 [01:16<16:02,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05478128045797348, 2: 0.06879928018374257 \t Acc: 0.6707333327974966\n",
      "Train Loss :1.336034893989563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 31/276 [01:54<15:38,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.14948683977127075, 2: 0.14390677999920187 \t Acc: 0.6407743644975995\n",
      "Train Loss :1.3242474794387817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 41/276 [02:33<15:04,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2812322974205017, 2: 0.2043862253285373 \t Acc: 0.6526615681798696\n",
      "Train Loss :1.2989312410354614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 51/276 [03:11<14:30,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.14975519478321075, 2: 0.11487469356410787 \t Acc: 0.6788442268947188\n",
      "Train Loss :1.232873558998108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 61/276 [03:50<13:51,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10835190117359161, 2: 0.09215898662225717 \t Acc: 0.6067571025162894\n",
      "Train Loss :1.3888652324676514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 71/276 [04:29<13:15,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07865556329488754, 2: 0.09560733506833738 \t Acc: 0.6318905258701989\n",
      "Train Loss :1.3114343881607056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 77/276 [04:52<12:47,  3.86s/it]"
     ]
    }
   ],
   "source": [
    "# Run learn from scratch (UNCOMMENT TO RUN)\n",
    "\n",
    "t_loss, t_acc, t_history, v_loss, v_acc, v_history = learn_from_scratch(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
