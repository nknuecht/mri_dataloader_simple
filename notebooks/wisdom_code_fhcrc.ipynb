{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pprint\n",
    "import tqdm\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import KFold\n",
    "# from torchinfo import summary\n",
    "\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "import skimage.transform as skTrans\n",
    "from numpy import logical_and as l_and, logical_not as l_not\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproduciablity\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleToFixed(object):\n",
    "\n",
    "    def __init__(self, new_shape, interpolation=1, channels=4):\n",
    "        self.shape= new_shape\n",
    "        self.interpolation = interpolation\n",
    "        self.channels = channels\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # print('first shape', image.shape)\n",
    "        if image is not None: # (some patients don't have segmentations)\n",
    "            if self.channels == 1:\n",
    "                short_shape = (self.shape[1], self.shape[2], self.shape[3])\n",
    "                image = skTrans.resize(image, short_shape, order=self.interpolation, preserve_range=True)  #\n",
    "                image = image.reshape(self.shape)\n",
    "            else:\n",
    "                image = skTrans.resize(image, self.shape, order=self.interpolation, preserve_range=True)  #\n",
    "\n",
    "        # print('second shape', image.shape)\n",
    "        # print()\n",
    "        return image\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \"\"\"Randomly flips (horizontally as well as vertically) the given PIL.Image with a probability of 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, prob_flip=0.5):\n",
    "        self.prob_flip= prob_flip\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if random.random() < self.prob_flip:\n",
    "            flip_type = np.random.randint(0, 3) # flip across any 3D axis\n",
    "            image = np.flip(image, flip_type)\n",
    "        return image\n",
    "\n",
    "class ZeroChannel(object):\n",
    "    \"\"\"Randomly sets channel to zero the given PIL.Image with a probability of 0.25\n",
    "    \"\"\"\n",
    "    def __init__(self, prob_zero=0.25, channels=4):\n",
    "        self.prob_zero= prob_zero\n",
    "        self.channels = channels\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if np.random.random() < self.prob_zero:\n",
    "            channel_to_zero = np.random.randint(0, self.channels) # flip across any 3D axis\n",
    "            zeros = np.zeros((image.shape[1], image.shape[2], image.shape[3]))\n",
    "            image[channel_to_zero, :, :, :] = zeros\n",
    "        return image\n",
    "\n",
    "class ZeroSprinkle(object):\n",
    "    def __init__(self, prob_zero=0.25, prob_true=0.5, channels=4):\n",
    "        self.prob_zero=prob_zero\n",
    "        self.prob_true=prob_true\n",
    "        self.channels=channels\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if self.prob_true:\n",
    "            mask = np.random.rand(image.shape[0], image.shape[1], image.shape[2], image.shape[3])\n",
    "            mask[mask < self.prob_zero] = 0\n",
    "            mask[mask > 0] = 1\n",
    "            image = image*mask\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "class MinMaxNormalize(object):\n",
    "    \"\"\"Min-Max normalization\n",
    "    \"\"\"\n",
    "    def __call__(self, image):\n",
    "        def norm(im):\n",
    "            im = im.astype(np.float32)\n",
    "            min_v = np.min(im)\n",
    "            max_v = np.max(im)\n",
    "            im = (im - min_v)/(max_v - min_v)\n",
    "            return im\n",
    "        image = norm(image)\n",
    "        return image\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, scale=1):\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if image is not None:\n",
    "            image = image.astype(np.float32)\n",
    "            image = image.reshape((image.shape[0], int(image.shape[1]/self.scale), int(image.shape[2]/self.scale), int(image.shape[3]/self.scale)))\n",
    "            image_tensor = torch.from_numpy(image)\n",
    "            return image_tensor\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for i, t in enumerate(self.transforms):\n",
    "            image = t(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bb_3D(img, pad=0):\n",
    "    '''\n",
    "    This function returns a tumor 3D bounding box using a segmentation mask\n",
    "    '''\n",
    "    xs = np.nonzero(np.sum(np.sum(img, axis=1), axis=1))\n",
    "    ys = np.nonzero(np.sum(np.sum(img, axis=0), axis=1))\n",
    "    zs = np.nonzero(np.sum(np.sum(img, axis=0), axis=0))\n",
    "    xmin, xmax = np.min(xs), np.max(xs)\n",
    "    ymin, ymax = np.min(ys), np.max(ys)\n",
    "    zmin, zmax = np.min(zs), np.max(zs)\n",
    "    bbox = (xmin-pad, ymin-pad, zmin-pad, xmax+pad, ymax+pad, zmax+pad)\n",
    "    return bbox\n",
    "\n",
    "def min_max(img):\n",
    "    '''\n",
    "    Min-max normalization\n",
    "    '''\n",
    "    return (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "def read_mri(mr_path_dict, pad=0):\n",
    "\n",
    "    image_shape = nib.load(mr_path_dict['flair']).get_fdata().shape\n",
    "    bb_seg = get_bb_3D(nib.load(mr_path_dict['flair']).get_fdata())\n",
    "    (xmin, ymin, zmin, xmax, ymax, zmax) = bb_seg\n",
    "\n",
    "    xmin = np.max([0, xmin-pad])\n",
    "    ymin = np.max([0, ymin-pad])\n",
    "    zmin = np.max([0, zmin-pad])\n",
    "\n",
    "    xmax = np.min([image_shape[0]-1, xmax+pad])\n",
    "    ymax = np.min([image_shape[1]-1, ymax+pad])\n",
    "    zmax = np.min([image_shape[2]-1, zmax+pad])\n",
    "\n",
    "\n",
    "    img_dict = {}\n",
    "    for key in ['flair', 't1', 't1ce', 't2', 'seg']:\n",
    "        img = nib.load(mr_path_dict[key])\n",
    "        img_data = img.get_fdata()\n",
    "        img_dict[key] = img_data[xmin:xmax, ymin:ymax, zmin:zmax]\n",
    "\n",
    "    stacked_img = np.stack([min_max(img_dict['flair']), min_max(img_dict['t1']),min_max(img_dict['t1ce']),min_max(img_dict['t2'])], axis=0)\n",
    "    return stacked_img, img_dict['seg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_(image, seg, predicted=False):\n",
    "    #Overlay with Predicted\n",
    "    img = image[slice, :, :, :].squeeze()\n",
    "    img = utils.make_grid(img)\n",
    "    img = img.detach().cpu().numpy()\n",
    "    \n",
    "    print(img.shape)\n",
    "    \n",
    "    # plot images\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    img_list = [img[i].T for i in range(channels)] # 1 image per channel\n",
    "    plt.imshow(np.hstack(img_list), cmap='Greys_r')\n",
    "    \n",
    "    ## plot segmentation mask ##\n",
    "    seg_img = torch.tensor(pred[slice].squeeze())\n",
    "    if not predicted:\n",
    "        seg_img = torch.tensor(seg_img.numpy()[:, ::-1].copy()) #flip\n",
    "    seg_img = utils.make_grid(seg_img).detach().cpu().numpy()\n",
    "    \n",
    "    print(np.unique(seg_img))\n",
    "\n",
    "    plt.imshow(np.hstack([seg_img[0].T]), cmap='Greys_r', alpha=0.3)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                metadata_df,\n",
    "                root_dir,\n",
    "                transform=None,\n",
    "                seg_transform=None, ###\n",
    "                dataformat=None, # indicates what shape (or content) should be returned (2D or 3D, etc.)\n",
    "                returndims=None, # what size/shape 3D volumes should be returned as.\n",
    "                output_shape=None,\n",
    "                visualize=False,\n",
    "                modality=None,\n",
    "                pad=2,\n",
    "                device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metadata_df (string): Path to the csv file w/ patient IDs\n",
    "            root_dir (string): Directory for MR images\n",
    "            transform (callable, optional)\n",
    "        \"\"\"\n",
    "        self.device=device\n",
    "        self.metadata_df = metadata_df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.seg_transform = seg_transform\n",
    "        self.returndims=returndims\n",
    "        self.modality = modality\n",
    "        self.pad = pad\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(type(idx), idx)\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        BraTS20ID = self.metadata_df.iloc[idx].BraTS_2020_subject_ID\n",
    "\n",
    "        # make dictonary of paths to MRI volumnes (modalities) and segmenation masks\n",
    "        mr_path_dict = {}\n",
    "        sequence_type = ['seg', 't1', 't1ce', 'flair', 't2']\n",
    "        for seq in sequence_type:\n",
    "            mr_path_dict[seq] = os.path.join(self.root_dir, BraTS20ID, BraTS20ID + '_'+seq+'.nii.gz')\n",
    "\n",
    "        image, seg_image = read_mri(mr_path_dict=mr_path_dict, pad=self.pad)\n",
    "        \n",
    "        if seg_image is not None:\n",
    "            if self.output_shape == 'wt':\n",
    "                seg_image[np.nonzero(seg_image)] = 1 #only 0's and 1's for background and tumor\n",
    "            else:\n",
    "                seg_image[seg_image == 4] = 3 #0,1,2,3 for background and tumor regions\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.seg_transform:\n",
    "            seg_image = self.seg_transform(seg_image)\n",
    "        else:\n",
    "            print('no transform')\n",
    "        # print(image.shape)\n",
    "        return (image, seg_image), BraTS20ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(params):\n",
    "    if platform.system() == 'Windows':\n",
    "        naming = pd.read_csv(f\"{params['image_dir']}\\\\name_mapping.csv\")\n",
    "    else:\n",
    "        naming = pd.read_csv(f\"{params['image_dir']}/name_mapping.csv\")\n",
    "    \n",
    "    data_df = pd.DataFrame(naming['BraTS_2020_subject_ID'])\n",
    "\n",
    "    # n_patients_to_train_with\n",
    "    total_num_patients = len(data_df)\n",
    "\n",
    "    assert sum(params['tr_va_te_split']) == 100\n",
    "    tr_split = int((total_num_patients * params['tr_va_te_split'][0]) / 100)\n",
    "    va_split = int((total_num_patients * params['tr_va_te_split'][1]) / 100)\n",
    "    te_split = total_num_patients - (tr_split + va_split)\n",
    "\n",
    "    print(f\"Data is split into train: {tr_split}, validation: {va_split} and test: {te_split}\")\n",
    "                   \n",
    "    train_df = data_df[: tr_split]\n",
    "    valid_df = data_df[tr_split : (tr_split + va_split)]\n",
    "    test_df = data_df[(tr_split + va_split) :]\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dataset(df, train=False):\n",
    "\n",
    "    image_dir, channels, resize_shape, output_shape = params['image_dir'], \\\n",
    "                                                      params['channels'], \\\n",
    "                                                      params['resize_shape'], \\\n",
    "                                                      params['output_shape']\n",
    "\n",
    "    # basic data augmentation\n",
    "    prob_voxel_zero = 0 # 0.1\n",
    "    prob_channel_zero = 0 # 0.5\n",
    "    prob_true = 0 # 0.8\n",
    "    randomflip = RandomFlip()\n",
    "\n",
    "    # MRI transformations\n",
    "    train_transformations = Compose([\n",
    "        MinMaxNormalize(),\n",
    "        ScaleToFixed((channels, resize_shape[0], resize_shape[1],\n",
    "                      resize_shape[2]), interpolation=1, channels=channels),\n",
    "        ZeroSprinkle(prob_zero=prob_voxel_zero, prob_true=prob_true),\n",
    "        ZeroChannel(prob_zero=prob_channel_zero),\n",
    "        randomflip,\n",
    "        ToTensor()\n",
    "        ])\n",
    "    \n",
    "    val_transformations = Compose([\n",
    "            MinMaxNormalize(),\n",
    "            ScaleToFixed((channels, resize_shape[0], resize_shape[1],\n",
    "                          resize_shape[2]), interpolation=1, channels=channels),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "\n",
    "    # GT segmentation mask transformations\n",
    "    seg_transformations = Compose([\n",
    "        ScaleToFixed((1, resize_shape[0], resize_shape[1],\n",
    "                      resize_shape[2]), interpolation=0, channels=1),\n",
    "        randomflip,\n",
    "        ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    if train:\n",
    "        dataset = GeneralDataset(metadata_df=df, \n",
    "                                root_dir=image_dir,\n",
    "                                transform=train_transformations,\n",
    "                                seg_transform=seg_transformations,\n",
    "                                returndims=resize_shape,\n",
    "                                output_shape=output_shape)\n",
    "    else:\n",
    "        dataset = GeneralDataset(metadata_df=df, \n",
    "                                root_dir=image_dir,\n",
    "                                transform=val_transformations,\n",
    "                                seg_transform=seg_transformations,\n",
    "                                returndims=resize_shape,\n",
    "                                output_shape=output_shape)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(params):\n",
    "\n",
    "    train_df, valid_df, test_df = read_dataframe(params)\n",
    "\n",
    "    train_dataset = retrieve_dataset(train_df, train=True)\n",
    "    \n",
    "    valid_dataset = retrieve_dataset(valid_df)\n",
    "\n",
    "    test_dataset = retrieve_dataset(test_df)\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['train_batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=params['train_batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['test_batch_size'])\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, norm='b', num_groups=2, k_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv3d = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=k_size,\n",
    "                                stride=stride, padding=padding)\n",
    "        if norm == 'b':\n",
    "            self.norm = nn.BatchNorm3d(num_features=out_channels)\n",
    "        else:\n",
    "            # use only one group if the given number of groups is greater than the number of channels\n",
    "            if out_channels < num_groups:\n",
    "                num_groups = 1\n",
    "            assert out_channels % num_groups == 0, f'Expected out_channels{out_channels} in input to be divisible by num_groups{num_groups}'\n",
    "            self.norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(self.conv3d(x))\n",
    "        x = F.elu(x) #!\n",
    "        return x\n",
    "\n",
    "class ConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k_size=3, stride=2, padding=1, output_padding=1):\n",
    "        super(ConvTranspose, self).__init__()\n",
    "        self.conv3d_transpose = nn.ConvTranspose3d(in_channels=in_channels,\n",
    "                                                   out_channels=out_channels,\n",
    "                                                   kernel_size=k_size,\n",
    "                                                   stride=stride,\n",
    "                                                   padding=padding,\n",
    "                                                   output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv3d_transpose(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, init_features, norm='b', num_groups=2, model_depth=4, pool_size=2):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.root_feat_maps = init_features\n",
    "        self.num_conv_blocks = 2\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "        for depth in range(model_depth):\n",
    "            feat_map_channels = 2 ** (depth + 1) * self.root_feat_maps\n",
    "            for i in range(self.num_conv_blocks):\n",
    "                self.conv_block = ConvBlock(in_channels=in_channels, out_channels=feat_map_channels, norm=norm, num_groups=num_groups)\n",
    "                self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv_block\n",
    "                in_channels, feat_map_channels = feat_map_channels, feat_map_channels * 2\n",
    "            if depth == model_depth - 1:\n",
    "                break\n",
    "            else:\n",
    "                self.pooling = nn.MaxPool3d(kernel_size=pool_size, stride=2, padding=0)\n",
    "                self.module_dict[\"max_pooling_{}\".format(depth)] = self.pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_sampling_features = []\n",
    "        for k, op in self.module_dict.items():\n",
    "            if k.startswith(\"conv\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "                if k.endswith(\"1\"):\n",
    "                    down_sampling_features.append(x)\n",
    "            elif k.startswith(\"max_pooling\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "\n",
    "        return x, down_sampling_features\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels, init_features, norm, num_groups=2, model_depth=4):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.num_conv_blocks = 2\n",
    "        self.num_feat_maps = init_features\n",
    "        # user nn.ModuleDict() to store ops\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "\n",
    "        for depth in range(model_depth - 2, -1, -1):\n",
    "            # print(depth)\n",
    "            feat_map_channels = 2 ** (depth + 1) * self.num_feat_maps\n",
    "            # print(feat_map_channels * 4)\n",
    "            self.deconv = ConvTranspose(in_channels=feat_map_channels * 4, out_channels=feat_map_channels * 4)\n",
    "            self.module_dict[\"deconv_{}\".format(depth)] = self.deconv\n",
    "            for i in range(self.num_conv_blocks):\n",
    "                if i == 0:\n",
    "                    self.conv = ConvBlock(in_channels=feat_map_channels * 6, out_channels=feat_map_channels * 2, norm=norm, num_groups=num_groups)\n",
    "                    self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv\n",
    "                else:\n",
    "                    self.conv = ConvBlock(in_channels=feat_map_channels * 2, out_channels=feat_map_channels * 2, norm=norm, num_groups=num_groups)\n",
    "                    self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv\n",
    "            if depth == 0:\n",
    "                self.final_conv = ConvBlock(in_channels=feat_map_channels * 2, out_channels=out_channels, norm=norm, num_groups=num_groups)\n",
    "                self.module_dict[\"final_conv\"] = self.final_conv\n",
    "\n",
    "    def forward(self, x, down_sampling_features):\n",
    "        \"\"\"\n",
    "        :param x: inputs\n",
    "        :param down_sampling_features: feature maps from encoder path\n",
    "        :return: output\n",
    "        \"\"\"\n",
    "        for k, op in self.module_dict.items():\n",
    "            if k.startswith(\"deconv\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "                x = torch.cat((down_sampling_features[int(k[-1])], x), dim=1)\n",
    "            elif k.startswith(\"conv\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "            else:\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, init_features, norm, num_groups=2, model_depth=4, final_activation=\"sigmoid\"):\n",
    "        super(UnetModel, self).__init__()\n",
    "        self.encoder = EncoderBlock(in_channels=in_channels,\n",
    "                                    init_features=init_features,\n",
    "                                    norm=norm, num_groups=num_groups,\n",
    "                                    model_depth=model_depth)\n",
    "        self.decoder = DecoderBlock(out_channels=out_channels,\n",
    "                                    init_features=init_features,\n",
    "                                    norm=norm, num_groups=num_groups,\n",
    "                                    model_depth=model_depth)\n",
    "        if final_activation == \"sigmoid\":\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        else:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, downsampling_features = self.encoder(x)\n",
    "        x = self.decoder(x, downsampling_features)\n",
    "        x = self.sigmoid(x)\n",
    "        # print(\"Final output shape: \", x.shape)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        # smooth factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        batch_size = targets.size(0)\n",
    "        # log_prob = torch.sigmoid(logits)\n",
    "        logits = logits.view(batch_size, -1).type(torch.FloatTensor)\n",
    "        targets = targets.view(batch_size, -1).type(torch.FloatTensor)\n",
    "        intersection = (logits * targets).sum(-1)\n",
    "        dice_score = 2. * intersection / ((logits + targets).sum(-1) + self.epsilon)\n",
    "        # dice_score = 1 - dice_score.sum() / batch_size\n",
    "        dice_score = torch.mean(1. - dice_score)\n",
    "        dice_score.requires_grad = True\n",
    "        return dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss tailored to Brats need.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, do_sigmoid=True):\n",
    "        super(EDiceLoss, self).__init__()\n",
    "        self.do_sigmoid = do_sigmoid\n",
    "        self.labels = [\"ET\", \"TC\", \"WT\"]\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "    def binary_dice(self, inputs, targets, label_index, metric_mode=False):\n",
    "        smooth = 1.\n",
    "        if self.do_sigmoid:\n",
    "            inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        if metric_mode:\n",
    "            inputs = inputs > 0.5\n",
    "            if targets.sum() == 0:\n",
    "                print(f\"No {self.labels[label_index]} for this patient\")\n",
    "                if inputs.sum() == 0:\n",
    "                    return torch.tensor(1., device=device)\n",
    "                else:\n",
    "                    return torch.tensor(0., device=device)\n",
    "            # Threshold the pred\n",
    "        intersection = EDiceLoss.compute_intersection(inputs, targets)\n",
    "        if metric_mode:\n",
    "            dice = (2 * intersection) / ((inputs.sum() + targets.sum()) * 1.0)\n",
    "        else:\n",
    "            dice = (2 * intersection + smooth) / (inputs.pow(2).sum() + targets.pow(2).sum() + smooth)\n",
    "        if metric_mode:\n",
    "            return dice\n",
    "        return 1 - dice\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_intersection(inputs, targets):\n",
    "        intersection = torch.sum(inputs * targets)\n",
    "        return intersection\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        dice = 0\n",
    "        for i in range(target.size(1)):\n",
    "            dice = dice + self.binary_dice(inputs[:, i, ...], target[:, i, ...], i)\n",
    "        final_dice = dice / target.size(1)\n",
    "        final_dice.requires_grad = True\n",
    "        return final_dice\n",
    "\n",
    "    def metric(self, inputs, target):\n",
    "        dices = []\n",
    "        for j in range(target.size(0)):\n",
    "            dice = []\n",
    "            for i in range(target.size(1)):\n",
    "                dice.append(self.binary_dice(inputs[j, i], target[j, i], i, True))\n",
    "            dices.append(dice)\n",
    "        return dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, targets, patient):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds:\n",
    "        torch tensor of size 1*C*Z*Y*X, ours BS*Z*Y*X \n",
    "    targets:\n",
    "        torch tensor of same shape\n",
    "    patient :\n",
    "        The patient ID\n",
    "    \"\"\"\n",
    "\n",
    "    assert preds.shape == targets.shape, \"Preds and targets do not have the same size\"\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    \n",
    "    preds, targets = preds.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
    "\n",
    "    metrics_list = []\n",
    "\n",
    "    metrics = dict(\n",
    "        patient_id=patient,\n",
    "    )\n",
    "    # print(targets.shape, targets.dtype, targets)\n",
    "    \n",
    "    if np.sum(targets) == 0:\n",
    "        print(f\"{label} not present for {patient}\")\n",
    "    else:\n",
    "        tp = np.sum(l_and(preds, targets))\n",
    "        tn = np.sum(l_and(l_not(preds), l_not(targets)))\n",
    "        fp = np.sum(l_and(preds, l_not(targets)))\n",
    "        fn = np.sum(l_and(l_not(preds), targets))\n",
    "\n",
    "        sens = tp / (tp + fn)\n",
    "        spec = tn / (tn + fp)\n",
    "        acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "        dice = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "    metrics[DICE] = dice\n",
    "    metrics[ACC] = acc\n",
    "    metrics[SENS] = sens\n",
    "    metrics[SPEC] = spec\n",
    "    # pp.pprint(metrics)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "    return acc, dice, metrics_list\n",
    "\n",
    "\n",
    "DICE = \"dice\"\n",
    "ACC = \"acc\"\n",
    "SENS = \"sens\"\n",
    "SPEC = \"spec\"\n",
    "METRICS = [DICE, ACC, SENS, SPEC]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dice(preds, targets):\n",
    "    return (2 * torch.sum(preds * targets)) / ((preds.sum() + preds.sum()) * 1.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_metric(train, label, metric_name):\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.semilogy(train, label=label)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'Model {metric_name} Plot')\n",
    "    plt.savefig(f'Model_{metric_name}_{label}_Plot.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def plot_result(kfolds, num_epochs, fold_train_history, fold_valid_history):\n",
    "    final_fold = {'train_loss':[],'valid_loss':[],'train_acc':[],'valid_acc':[]}\n",
    "\n",
    "    for epoch in range(num_epochs):                                      \n",
    "        final_fold['train_loss'].append(np.mean([fold_train_history[str(fold)]['train_loss'][epoch] for fold in range(kfolds)]))\n",
    "        final_fold['train_acc'].append(np.mean([fold_train_history[str(fold)]['train_acc'][epoch]for fold in range(kfolds)]))\n",
    "\n",
    "    plot_metric(final_fold['train_loss'], 'train', 'Loss')\n",
    "    plot_metric(final_fold['train_acc'], 'validation', 'Accuracy')\n",
    "\n",
    "    final_fold['valid_loss'].append([fold_valid_history[str(fold)]['valid_loss'] for fold in range(kfolds)])\n",
    "    final_fold['valid_acc'].append([fold_valid_history[str(fold)]['valid_acc'] for fold in range(kfolds)])\n",
    "\n",
    "    print(final_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, params):\n",
    "\n",
    "    test_model = UnetModel(params['pretrain_in_channels'],\n",
    "                           params['pretrain_out_channels'],\n",
    "                           params['init_features'], params['norm'],\n",
    "                           params['num_groups'],\n",
    "                           )\n",
    "\n",
    "    test_optimizer = torch.optim.AdamW(test_model.parameters(),\n",
    "                                       lr=params['learning_rate'],\n",
    "                                       )\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(path)\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    return test_model, test_optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_folds(model, optimizer, fold, epoch, loss):\n",
    "    # Saving the model\n",
    "    save_path = f'model-fold-{fold}.pth'\n",
    "\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': loss,\n",
    "                  }\n",
    "    torch.save(checkpoint, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_nofolds(model, optimizer, epoch, loss, params):\n",
    "    now = datetime.now()\n",
    "    # dd/mm/YY H:M:S\n",
    "    dt_string = now.strftime(\"%d_%H_%M\")\n",
    "\n",
    "    # Saving the model\n",
    "    save_path = f\"model_{params['output_shape']}_{params['run_name']}_{dt_string}.pth\"\n",
    "\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': loss,\n",
    "                  'params': params,\n",
    "                  }\n",
    "    torch.save(checkpoint, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldRunAll(criterion, dataset, params): \n",
    "                \n",
    "    k_folds, num_epochs, train_batch_size = params['k_folds'],\\\n",
    "                                            params['no_epochs'],\\\n",
    "                                            params['train_batch_size']\n",
    "    \n",
    "    use_cuda, loss_name, in_channels, out_channels = params['use_cuda'], \\\n",
    "                                                     params['loss_name'],\\\n",
    "                                                     params['in_channels'],\\\n",
    "                                                     params['out_channels'],\n",
    "\n",
    "    init_features, learning_rate, norm, num_groups = params['init_features'],\\\n",
    "                                                     params['learning_rate'],\\\n",
    "                                                     params['norm'], \\\n",
    "                                                     params['num_groups'],\n",
    "\n",
    "    loss_function = criterion\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    fold_train_history = {}\n",
    "    fold_valid_history = {}\n",
    "    fold_train_and_valid_acc = {}\n",
    "    fold_train_and_valid_loss = {}\n",
    "\n",
    "    print('--------------------------------')\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        valid_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        dataloader_train = DataLoader(dataset, batch_size=train_batch_size, sampler=train_subsampler, num_workers=0)\n",
    "        dataloader_valid = DataLoader(dataset, batch_size=train_batch_size, sampler=valid_subsampler, num_workers=0)\n",
    "\n",
    "        # Initialize optimizer and Model\n",
    "        model = UnetModel(in_channels=in_channels, out_channels=out_channels,\n",
    "                          init_features=init_features, norm=norm, num_groups=num_groups)\n",
    "        if use_cuda:\n",
    "            model = model.cuda()\n",
    "        #print(model)\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Run the training, testing and saving loop for defined number of epochs\n",
    "        start_time = time.time()\n",
    "\n",
    "        t_loss, t_acc, t_history, v_loss, v_acc, v_history = train_test_folds(model, loss_function, optimizer,\n",
    "                                                                               dataloader_train, dataloader_valid,\n",
    "                                                                               fold, num_epochs, use_cuda,\n",
    "                                                                               loss_name)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Epoch Time: {end_time - start_time}\")\n",
    "\n",
    "        #Saving loss results \n",
    "        fold_train_and_valid_loss[str(fold)] = [t_loss, v_loss]\n",
    "        fold_train_and_valid_acc[str(fold)] = [t_acc, v_acc]\n",
    "        fold_train_history[str(fold)] = t_history\n",
    "        fold_valid_history[str(fold)] = v_history\n",
    "\n",
    "        # Print accuracy\n",
    "        print(f'Accuracy for fold {fold}: {v_acc}')\n",
    "        print(f'Loss for fold {fold}: {v_loss}')\n",
    "        print('--------------------------------')  \n",
    "\n",
    "    return fold_train_history, fold_valid_history, fold_train_and_valid_loss, fold_train_and_valid_acc\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_folds(model, loss_function, optimizer, dataloader_train, dataloader_valid, fold, num_epochs, use_cuda, loss_name):\n",
    "    train_history = {'train_loss': [], 'train_acc':[], 'train_dice':[]}\n",
    "    valid_history = {'valid_loss': [], 'valid_acc':[], 'valid_dice':[]}\n",
    "    best = math.inf\n",
    "\n",
    "    edice = EDiceLoss()\n",
    "    if use_cuda:\n",
    "        edice = edice.cuda()\n",
    "    metric = edice.metric\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Starting Train epoch: {epoch+1}')\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc, train_dice = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader_train)):\n",
    "            (inputs, targets), ID = data\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape, outputs.shape, targets.squeeze(1).long().shape)\n",
    "\n",
    "            if loss_name == 'dice':\n",
    "                class_outputs = outputs.argmax(dim=1)\n",
    "                loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "            else:\n",
    "                loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "            train_loss += loss.item() * outputs.size(0) #multiplying by batchsize\n",
    "            \n",
    "            rtrain_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "            rtrain_acc, rtrain_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "            print(f'Train Dice 1: {rtrain_dice1}, 2: {rtrain_dice2} \\t Acc: {rtrain_acc}')\n",
    "            train_acc += rtrain_acc\n",
    "            train_dice += rtrain_dice1\n",
    "            \n",
    "            print(f'Train Loss :{loss.item()}')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "        train_history['train_loss'].append(train_loss / len(dataloader_train.sampler))\n",
    "        train_history['train_acc'].append(train_acc / len(dataloader_train.sampler))\n",
    "        train_history['train_dice'].append(train_dice / len(dataloader_train.sampler))\n",
    "\n",
    "        print(f\"Train Epoch loss: {train_history['train_loss'][-1]}, \\t ACC/DICE :{train_history['train_acc'][-1]}/{train_history['train_dice'][-1]} \")\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    valid_acc, valid_dice = 0, 0\n",
    "           \n",
    "    model.eval()\n",
    "    #! maybe change later to validate after some epochs\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader_valid)):\n",
    "            (inputs, targets), ID = data\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda() \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if params['loss_name'] == 'dice':\n",
    "                class_outputs = outputs.argmax(dim=1)\n",
    "                loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "            else:\n",
    "                loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "            # print('Valid Loss:', loss.item())\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            rvalid_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "\n",
    "            rvalid_acc, rvalid_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "            # print(f'Val Dice 1: {rvalid_dice1}, 2: {rvalid_dice2}')\n",
    "            valid_acc += rvalid_acc\n",
    "            valid_dice += rvalid_dice1\n",
    "            \n",
    "        # Print accuracy\n",
    "        print(f'Val Dice : {valid_dice}, len {len(dataloader_valid.sampler)}')\n",
    "        valid_loss /= len(dataloader_valid.sampler) \n",
    "        valid_acc = valid_acc / len(dataloader_valid.sampler)\n",
    "        valid_dice = valid_dice / len(dataloader_valid.sampler)\n",
    "        #print(f\" Fold Accuracy: {valid_acc}\")\n",
    "\n",
    "    valid_history['valid_loss'].append(valid_loss)\n",
    "    valid_history['valid_acc'].append(valid_acc)\n",
    "    valid_history['valid_dice'].append(valid_dice)\n",
    "\n",
    "    print(f\"Val Epoch loss: {valid_history['valid_loss'][-1]} \\t acc/dice:/ {valid_history['valid_acc'][-1]}/ {valid_history['valid_dice'][-1]}\")\n",
    "\n",
    "    # saving best model for this fold\n",
    "    if valid_loss < best:\n",
    "        best = valid_loss\n",
    "        save_model_folds(model, optimizer, fold, epoch, loss)\n",
    "    \n",
    "    \n",
    "    return train_history['train_loss'][-1], train_history['train_acc'][-1], train_history, valid_history['valid_loss'][-1], valid_history['valid_acc'][-1], valid_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_nofolds(model, loss_function, optimizer, dataloader_train, dataloader_valid, params):\n",
    "    train_history = {'train_loss': [], 'train_acc':[], 'train_dice':[]}\n",
    "    valid_history = {'valid_loss': [], 'valid_acc':[], 'valid_dice':[]}\n",
    "    best = math.inf\n",
    "\n",
    "    edice = EDiceLoss()\n",
    "    if params['use_cuda']:\n",
    "        edice = edice.cuda()\n",
    "    metric = edice.metric\n",
    "\n",
    "    #Automatic mixed precision addition\n",
    "    #scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "    for epoch in range(params['no_epochs']):\n",
    "        print(f'Starting Train epoch: {epoch+1}')\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc, train_dice = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader_train)):\n",
    "            (inputs, targets), ID = data\n",
    "            if params['use_cuda']:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #with torch.cuda.amp.autocast(enabled=params['autocast']):\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape, outputs.shape, targets.squeeze(1).long().shape)\n",
    "\n",
    "            if params['loss_name'] == 'dice':\n",
    "                class_outputs = outputs.argmax(dim=1)\n",
    "                loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "            else:\n",
    "                loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "            train_loss += loss.item() * outputs.size(0) # multiplying by batchsize\n",
    "            \n",
    "            rtrain_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "            rtrain_acc, rtrain_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Train Dice 1: {rtrain_dice1}, 2: {rtrain_dice2} \\t Acc: {rtrain_acc}')\n",
    "\n",
    "            train_acc += rtrain_acc\n",
    "            train_dice += rtrain_dice1\n",
    "            if i % 10 == 0:\n",
    "                print(f'Train Loss :{loss.item()}')\n",
    "            \n",
    "            #scaler.scale(loss).backward()\n",
    "            #scaler.step(optimizer)\n",
    "            #scaler.update()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "        train_history['train_loss'].append(train_loss / len(dataloader_train.sampler))\n",
    "        train_history['train_acc'].append(train_acc / len(dataloader_train.sampler))\n",
    "        train_history['train_dice'].append(train_dice / len(dataloader_train.sampler))\n",
    "\n",
    "        print(f\"Train Epoch loss: {train_history['train_loss'][-1]}, \\t ACC/DICE :{train_history['train_acc'][-1]}/{train_history['train_dice'][-1]} \")\n",
    "          \n",
    "        if epoch % (.1 * params['no_epochs']) == 0:\n",
    "            valid_loss = 0.0\n",
    "            valid_acc, valid_dice = 0, 0\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Iterate over the test data and generate predictions\n",
    "                for i, data in enumerate(tqdm.tqdm(dataloader_valid)):\n",
    "                    (inputs, targets), ID = data\n",
    "                    if params['use_cuda']:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda() \n",
    "                      \n",
    "                    #with torch.cuda.amp.autocast(enabled=params['autocast']):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    if params['loss_name'] == 'dice':\n",
    "                        class_outputs = outputs.argmax(dim=1)\n",
    "                        loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                        # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "                    else:\n",
    "                        loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "                    # print('Valid Loss:', loss.item())\n",
    "                    valid_loss += loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    rvalid_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "\n",
    "                    rvalid_acc, rvalid_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "                    # print(f'Val Dice 1: {rvalid_dice1}, 2: {rvalid_dice2}')\n",
    "                    valid_acc += rvalid_acc\n",
    "                    valid_dice += rvalid_dice1\n",
    "                    \n",
    "                # Print accuracy\n",
    "                print(f'Val Dice : {valid_dice}, len {len(dataloader_valid.sampler)}')\n",
    "                valid_loss /= len(dataloader_valid.sampler) \n",
    "                valid_acc = valid_acc / len(dataloader_valid.sampler)\n",
    "                valid_dice = valid_dice / len(dataloader_valid.sampler)\n",
    "\n",
    "            valid_history['valid_loss'].append(valid_loss)\n",
    "            valid_history['valid_acc'].append(valid_acc)\n",
    "            valid_history['valid_dice'].append(valid_dice)\n",
    "\n",
    "            print(f\"Val Epoch loss: {valid_history['valid_loss'][-1]} \\t acc/dice:/ {valid_history['valid_acc'][-1]}/ {valid_history['valid_dice'][-1]}\")\n",
    "\n",
    "            # saving best model for this fold\n",
    "            if valid_loss < best:\n",
    "                best = valid_loss\n",
    "                save_model_nofolds(model, optimizer, epoch, loss, params)\n",
    "    \n",
    "    \n",
    "    return train_history['train_loss'][-1], train_history['train_acc'][-1], train_history, valid_history['valid_loss'][-1], valid_history['valid_acc'][-1], valid_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfolds(params):\n",
    "    \n",
    "    if params['loss_name'] == 'ce':\n",
    "        criterion = CrossEntropyLoss()\n",
    "    elif params['loss_name'] == 'wce':\n",
    "        criterion = CrossEntropyLoss(weight=torch.Tensor([1, 355.36116969, 74.37872817, 254.58104099]))\n",
    "    else:\n",
    "        criterion = EDiceLoss().cuda()\n",
    "      \n",
    "    if params['use_cuda']:\n",
    "        criterion.cuda()\n",
    "    \n",
    "    train_df, valid_df, _ = read_dataframe(params)\n",
    "\n",
    "    train_valid_df = pd.concat([train_df, valid_df])\n",
    "\n",
    "    train_valid_dataset = retrieve_dataset(train_valid_df)\n",
    "\n",
    "    t_history, v_history, tv_loss, tv_acc = kFoldRunAll(criterion, \n",
    "                                                        train_valid_dataset,\n",
    "                                                        params)\n",
    "    \n",
    "    plot_result(k_folds, no_epochs, t_history, v_history)\n",
    "\n",
    "    return t_history, v_history, tv_loss, tv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(path, params):\n",
    "    \n",
    "    if params['loss_name'] == 'ce':\n",
    "        criterion = CrossEntropyLoss()\n",
    "    elif params['loss_name'] == 'wce':\n",
    "        criterion = CrossEntropyLoss(weight=torch.Tensor([1, 355.36116969, 74.37872817, 254.58104099]))\n",
    "    else:\n",
    "        criterion = EDiceLoss().cuda()\n",
    "    \n",
    "    model, optimizer, _, __ = load_checkpoint(path, params)\n",
    "\n",
    "    # Transfer by changing(replacing) only last layer and finetuning to outdim=4\n",
    "    model.decoder.final_conv = ConvBlock(in_channels=params['pretrain_in_final_conv'], \n",
    "                                          out_channels=params['out_channels'],\n",
    "                                          norm=params['norm'],\n",
    "                                          num_groups=params['num_groups'])\n",
    "\n",
    "    model.decoder.module_dict.final_conv = ConvBlock(in_channels=params['pretrain_in_final_conv'], \n",
    "                                          out_channels=params['out_channels'],\n",
    "                                          norm=params['norm'],\n",
    "                                          num_groups=params['num_groups'])\n",
    "\n",
    "    if params['use_cuda']:\n",
    "        model.cuda()\n",
    "        criterion.cuda()\n",
    "    \n",
    "    dataloader_train, dataloader_valid, _ = get_data(params)\n",
    "    \n",
    "    t_loss, t_acc, t_history, v_loss, v_acc, v_history = train_test_nofolds(\n",
    "                                                model, criterion, optimizer,\n",
    "                                                dataloader_train, dataloader_valid,\n",
    "                                                params\n",
    "                                                )\n",
    "\n",
    "    # plot_result(k_folds, params['no_epochs'], t_history, v_history)\n",
    "\n",
    "    return t_loss, t_acc, t_history, v_loss, v_acc, v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_from_scratch(params):\n",
    "    \n",
    "    if params['loss_name'] == 'ce':\n",
    "        criterion = CrossEntropyLoss()\n",
    "    elif params['loss_name'] == 'wce':\n",
    "        criterion = CrossEntropyLoss(weight=torch.Tensor([1, 355.36116969, 74.37872817, 254.58104099]))\n",
    "    else:\n",
    "        criterion = EDiceLoss()\n",
    "    \n",
    "\n",
    "\n",
    "    model = UnetModel(params['in_channels'],\n",
    "                      params['out_channels'],\n",
    "                      params['init_features'],\n",
    "                      params['norm'],\n",
    "                      params['num_groups'],\n",
    "                      )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=params['learning_rate'],\n",
    "                                  )\n",
    "\n",
    "    if params['use_cuda']:\n",
    "        model.cuda()\n",
    "        criterion.cuda()\n",
    "    \n",
    "    dataloader_train, dataloader_valid, _ = get_data(params)\n",
    "    \n",
    "    t_loss, t_acc, t_history, v_loss, v_acc, v_history = train_test_nofolds(\n",
    "                                                model, criterion, optimizer,\n",
    "                                                dataloader_train, dataloader_valid,\n",
    "                                                params\n",
    "                                                )\n",
    "\n",
    "    # plot_result(k_folds, params['no_epochs'], t_history, v_history)\n",
    "\n",
    "    return t_loss, t_acc, t_history, v_loss, v_acc, v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "\n",
    "    if platform.system() == 'Windows':\n",
    "        image_dir = r\"C:\\Users\\wisdomik\\Documents\\project\\MICCAI_BraTS2020_TrainingData\"\n",
    "    else:\n",
    "        image_dir = '../../data/data/mri/MICCAI_BraTS2020_TrainingData/'\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    params = {'run_name': 'run_1',\n",
    "              'in_channels': 4,\n",
    "              'out_channels': 4,\n",
    "              'no_epochs': 1,\n",
    "              'k_folds': 5,\n",
    "              'learning_rate': 1e-4,\n",
    "              'loss_name': 'wce',\n",
    "              'output_shape': 'all',\n",
    "              'tr_va_te_split': [75, 25, 0],\n",
    "              'pretrain_in_channels': 4,\n",
    "              'pretrain_out_channels': 2,\n",
    "              'pretrain_in_final_conv': 16,\n",
    "              'init_features': 8,\n",
    "              'train_batch_size': 1,\n",
    "              'autocast': False, # not in use\n",
    "              'test_batch_size': 1,\n",
    "              'norm': 'g',\n",
    "              'num_groups': 4,\n",
    "              'channels': 4,\n",
    "              'resize_shape': (128, 128, 128),\n",
    "              'image_dir': image_dir,\n",
    "              'use_cuda': use_cuda\n",
    "              }\n",
    "              \n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_name': 'run_1',\n",
       " 'in_channels': 4,\n",
       " 'out_channels': 4,\n",
       " 'no_epochs': 1,\n",
       " 'k_folds': 5,\n",
       " 'learning_rate': 0.0001,\n",
       " 'loss_name': 'wce',\n",
       " 'output_shape': 'all',\n",
       " 'tr_va_te_split': [75, 25, 0],\n",
       " 'pretrain_in_channels': 4,\n",
       " 'pretrain_out_channels': 2,\n",
       " 'pretrain_in_final_conv': 16,\n",
       " 'init_features': 8,\n",
       " 'train_batch_size': 1,\n",
       " 'autocast': False,\n",
       " 'test_batch_size': 1,\n",
       " 'norm': 'g',\n",
       " 'num_groups': 4,\n",
       " 'channels': 4,\n",
       " 'resize_shape': (128, 128, 128),\n",
       " 'image_dir': '../../data/data/mri/MICCAI_BraTS2020_TrainingData/',\n",
       " 'use_cuda': True}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set params then run this\n",
    "\n",
    "#load in trained model for evaluation\n",
    "# fold_to_check = 9\n",
    "# PATH = f'drive/MyDrive/Colab Notebooks/model-fold-{fold_to_check}.pth'\n",
    "\n",
    "params = get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/276 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is split into train: 276, validation: 92 and test: 1\n",
      "Starting Train epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/276 [00:02<12:28,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.032238684594631195, 2: 0.04294479606703858 \t Acc: 0.22164011001586914\n",
      "Train Loss :1.4165858030319214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 2/276 [00:06<14:52,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03821874409914017, 2: 0.05389257950530035 \t Acc: 0.36163806915283203\n",
      "Train Loss :1.4056919813156128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 3/276 [00:09<13:53,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.0693030059337616, 2: 0.09814075576126262 \t Acc: 0.43248653411865234\n",
      "Train Loss :1.42372727394104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 4/276 [00:11<13:23,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.0425037145614624, 2: 0.054177816691535846 \t Acc: 0.4514155387878418\n",
      "Train Loss :1.3172378540039062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 5/276 [00:15<14:30,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.12048594653606415, 2: 0.14569366318252921 \t Acc: 0.4005126953125\n",
      "Train Loss :1.4001191854476929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 6/276 [00:18<13:44,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11704041063785553, 2: 0.11272483381568321 \t Acc: 0.42533063888549805\n",
      "Train Loss :1.3813878297805786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 7/276 [00:21<13:06,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.256578266620636, 2: 0.18946459510731634 \t Acc: 0.45481395721435547\n",
      "Train Loss :1.2816311120986938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 8/276 [00:24<13:53,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2299259752035141, 2: 0.19034140560725354 \t Acc: 0.4565858840942383\n",
      "Train Loss :1.3590397834777832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 9/276 [00:27<13:14,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09790834784507751, 2: 0.09978119485596987 \t Acc: 0.4606938362121582\n",
      "Train Loss :1.3536851406097412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▎         | 10/276 [00:29<12:45,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10110199451446533, 2: 0.1012647608270453 \t Acc: 0.5606951713562012\n",
      "Train Loss :1.3341515064239502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 11/276 [00:32<12:34,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05736158788204193, 2: 0.07602512541979352 \t Acc: 0.5749406814575195\n",
      "Train Loss :1.3185280561447144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 12/276 [00:35<13:06,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09226773679256439, 2: 0.10600313654966873 \t Acc: 0.5960731506347656\n",
      "Train Loss :1.3078498840332031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 13/276 [00:39<13:57,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08679462224245071, 2: 0.0851507861251092 \t Acc: 0.5785961151123047\n",
      "Train Loss :1.3104546070098877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 14/276 [00:42<13:11,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.20370236039161682, 2: 0.17591236984453112 \t Acc: 0.6167221069335938\n",
      "Train Loss :1.3095195293426514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 15/276 [00:44<12:39,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1445893943309784, 2: 0.12751289264415502 \t Acc: 0.5832443237304688\n",
      "Train Loss :1.2993757724761963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 16/276 [00:48<13:30,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.06157253682613373, 2: 0.08266330742341452 \t Acc: 0.5680370330810547\n",
      "Train Loss :1.3187754154205322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 17/276 [00:51<12:52,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.02964138425886631, 2: 0.02543218848806078 \t Acc: 0.5549545288085938\n",
      "Train Loss :1.329577088356018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 18/276 [00:53<12:15,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.035810619592666626, 2: 0.0426551515311093 \t Acc: 0.5839042663574219\n",
      "Train Loss :1.3505849838256836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 19/276 [00:58<15:15,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05391969531774521, 2: 0.06469396063146783 \t Acc: 0.5935540199279785\n",
      "Train Loss :1.314012885093689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 20/276 [01:02<15:22,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.052867453545331955, 2: 0.09823413161265333 \t Acc: 0.6092252731323242\n",
      "Train Loss :1.2783105373382568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 21/276 [01:05<14:08,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.27461397647857666, 2: 0.2786629065432575 \t Acc: 0.6575145721435547\n",
      "Train Loss :1.291001796722412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 22/276 [01:07<13:15,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07458652555942535, 2: 0.12200838234636094 \t Acc: 0.5789608955383301\n",
      "Train Loss :1.3166663646697998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 23/276 [01:10<12:46,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11700263619422913, 2: 0.10174197705581876 \t Acc: 0.5699257850646973\n",
      "Train Loss :1.3277348279953003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▊         | 24/276 [01:13<12:10,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10055471956729889, 2: 0.1544808948022764 \t Acc: 0.6192827224731445\n",
      "Train Loss :1.298248052597046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 25/276 [01:16<11:48,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.20079940557479858, 2: 0.18057888487823087 \t Acc: 0.6064109802246094\n",
      "Train Loss :1.3204351663589478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 26/276 [01:21<14:49,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.19245599210262299, 2: 0.2670975634317086 \t Acc: 0.6251907348632812\n",
      "Train Loss :1.2835166454315186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 27/276 [01:23<13:40,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07573344558477402, 2: 0.07394162644690319 \t Acc: 0.6058483123779297\n",
      "Train Loss :1.3231651782989502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 28/276 [01:26<12:50,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11823894828557968, 2: 0.17350280346727676 \t Acc: 0.6086320877075195\n",
      "Train Loss :1.286097526550293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 29/276 [01:29<12:10,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10998803377151489, 2: 0.19414880258068837 \t Acc: 0.5868968963623047\n",
      "Train Loss :1.2846287488937378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 30/276 [01:31<11:41,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11105170100927353, 2: 0.10338843272322007 \t Acc: 0.6075558662414551\n",
      "Train Loss :1.315807819366455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 31/276 [01:34<11:26,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1501373052597046, 2: 0.2014561322282519 \t Acc: 0.6167488098144531\n",
      "Train Loss :1.2923977375030518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 32/276 [01:37<11:10,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03144967183470726, 2: 0.03982238771785949 \t Acc: 0.5662083625793457\n",
      "Train Loss :1.3207765817642212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 33/276 [01:39<11:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.22652171552181244, 2: 0.21692143509146383 \t Acc: 0.6130199432373047\n",
      "Train Loss :1.2999036312103271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 34/276 [01:42<10:50,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.04498417302966118, 2: 0.09996063660876756 \t Acc: 0.5780606269836426\n",
      "Train Loss :1.2784093618392944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 35/276 [01:45<10:43,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1162809282541275, 2: 0.12049289850051963 \t Acc: 0.5762777328491211\n",
      "Train Loss :1.3222882747650146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 36/276 [01:47<10:34,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09782511740922928, 2: 0.11458373286677662 \t Acc: 0.5707030296325684\n",
      "Train Loss :1.3163785934448242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 37/276 [01:50<10:40,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11465734988451004, 2: 0.11571749346049677 \t Acc: 0.544619083404541\n",
      "Train Loss :1.2886465787887573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 38/276 [01:53<10:46,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.06674108654260635, 2: 0.1274311659065687 \t Acc: 0.5890893936157227\n",
      "Train Loss :1.2770777940750122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 39/276 [01:55<10:37,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1504504531621933, 2: 0.17906951281738762 \t Acc: 0.6210222244262695\n",
      "Train Loss :1.277421474456787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 40/276 [01:59<11:33,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05358698219060898, 2: 0.08067014980346456 \t Acc: 0.55010986328125\n",
      "Train Loss :1.3073629140853882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▍        | 41/276 [02:01<10:59,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2899678349494934, 2: 0.25257572031252956 \t Acc: 0.6232213973999023\n",
      "Train Loss :1.3109629154205322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 42/276 [02:05<12:14,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07611244916915894, 2: 0.15166705735975622 \t Acc: 0.5962009429931641\n",
      "Train Loss :1.2493623495101929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 43/276 [02:08<11:37,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2613890469074249, 2: 0.18972774784264473 \t Acc: 0.6185312271118164\n",
      "Train Loss :1.3221999406814575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 44/276 [02:11<11:11,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03511171042919159, 2: 0.060517566337011074 \t Acc: 0.5769186019897461\n",
      "Train Loss :1.3033071756362915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▋        | 45/276 [02:13<10:57,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.193710595369339, 2: 0.2008632769844357 \t Acc: 0.5934624671936035\n",
      "Train Loss :1.3329975605010986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 46/276 [02:16<10:40,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.008259423077106476, 2: 0.010837061069327697 \t Acc: 0.5394315719604492\n",
      "Train Loss :1.3562748432159424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 47/276 [02:19<10:27,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.12600380182266235, 2: 0.12483486397467931 \t Acc: 0.6073555946350098\n",
      "Train Loss :1.3067632913589478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 48/276 [02:22<11:31,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.028193339705467224, 2: 0.04718515329793849 \t Acc: 0.5820512771606445\n",
      "Train Loss :1.3136541843414307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 49/276 [02:25<11:07,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10936244577169418, 2: 0.17951374712076976 \t Acc: 0.5937099456787109\n",
      "Train Loss :1.2479761838912964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 50/276 [02:28<10:41,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15680889785289764, 2: 0.29424969019997665 \t Acc: 0.648590087890625\n",
      "Train Loss :1.2158234119415283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 51/276 [02:31<11:39,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.01727902516722679, 2: 0.04103795260379057 \t Acc: 0.6099205017089844\n",
      "Train Loss :1.3312307596206665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 52/276 [02:34<10:57,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09740380197763443, 2: 0.1001908594561541 \t Acc: 0.6063656806945801\n",
      "Train Loss :1.2870851755142212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 53/276 [02:37<10:37,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.059994425624608994, 2: 0.07453578284873325 \t Acc: 0.5706033706665039\n",
      "Train Loss :1.3202948570251465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█▉        | 54/276 [02:41<12:08,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05364677682518959, 2: 0.0803351872989298 \t Acc: 0.5892939567565918\n",
      "Train Loss :1.281049132347107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█▉        | 55/276 [02:45<12:32,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.0500563345849514, 2: 0.08186059948826505 \t Acc: 0.5878071784973145\n",
      "Train Loss :1.380062222480774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 56/276 [02:51<15:51,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.02740693837404251, 2: 0.03006043357178951 \t Acc: 0.5917844772338867\n",
      "Train Loss :1.3233059644699097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 57/276 [02:54<13:59,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1109350398182869, 2: 0.11631145961614181 \t Acc: 0.5976133346557617\n",
      "Train Loss :1.3070387840270996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 58/276 [02:56<12:29,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2975292205810547, 2: 0.23980457759011498 \t Acc: 0.6129932403564453\n",
      "Train Loss :1.2329202890396118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 59/276 [03:01<13:58,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03843655064702034, 2: 0.04114199900007248 \t Acc: 0.5079965591430664\n",
      "Train Loss :1.3304567337036133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 60/276 [03:04<12:35,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.24882760643959045, 2: 0.19532495608701528 \t Acc: 0.5882353782653809\n",
      "Train Loss :1.2107754945755005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 61/276 [03:06<11:38,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.23252491652965546, 2: 0.19891790820270014 \t Acc: 0.5873351097106934\n",
      "Train Loss :1.1995532512664795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 62/276 [03:09<11:02,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.052909839898347855, 2: 0.06512365417130242 \t Acc: 0.49114227294921875\n",
      "Train Loss :1.2472050189971924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 63/276 [03:12<11:08,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10921042412519455, 2: 0.13835988525746792 \t Acc: 0.4066014289855957\n",
      "Train Loss :1.315247893333435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 64/276 [03:15<10:42,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09514334052801132, 2: 0.10586666534968368 \t Acc: 0.1367015838623047\n",
      "Train Loss :1.2979429960250854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▎       | 65/276 [03:19<11:52,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16433188319206238, 2: 0.12305898302060674 \t Acc: 0.4750180244445801\n",
      "Train Loss :1.22003972530365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 66/276 [03:22<11:02,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18096229434013367, 2: 0.14073814337225513 \t Acc: 0.37719011306762695\n",
      "Train Loss :1.281083583831787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 67/276 [03:25<10:26,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15561531484127045, 2: 0.14692616213181112 \t Acc: 0.4073958396911621\n",
      "Train Loss :1.2784898281097412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▍       | 68/276 [03:27<10:01,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08338932693004608, 2: 0.06692371228898425 \t Acc: 0.34444284439086914\n",
      "Train Loss :1.3029754161834717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 69/276 [03:30<09:44,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.04031975567340851, 2: 0.07697602769479586 \t Acc: 0.27556848526000977\n",
      "Train Loss :1.2715202569961548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 70/276 [03:33<09:36,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16271689534187317, 2: 0.14750230062108524 \t Acc: 0.38378477096557617\n",
      "Train Loss :1.3200029134750366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 71/276 [03:35<09:27,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08441214263439178, 2: 0.10630378594055578 \t Acc: 0.34956884384155273\n",
      "Train Loss :1.4235540628433228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 72/276 [03:38<09:07,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03731360659003258, 2: 0.05639017857363475 \t Acc: 0.27161121368408203\n",
      "Train Loss :1.2836627960205078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▋       | 73/276 [03:40<09:01,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11042815446853638, 2: 0.07770517204637295 \t Acc: 0.33754873275756836\n",
      "Train Loss :1.2764519453048706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 74/276 [03:45<10:25,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16456982493400574, 2: 0.20557208167930363 \t Acc: 0.4060640335083008\n",
      "Train Loss :1.3009974956512451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 75/276 [03:48<10:30,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.0608585886657238, 2: 0.06161079726390727 \t Acc: 0.2651195526123047\n",
      "Train Loss :1.398391604423523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 76/276 [03:52<11:35,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.12583661079406738, 2: 0.1397519991725246 \t Acc: 0.3892698287963867\n",
      "Train Loss :1.2996671199798584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 77/276 [03:58<13:52,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.06491302698850632, 2: 0.07230527214212489 \t Acc: 0.35576868057250977\n",
      "Train Loss :1.286017656326294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 78/276 [04:01<12:15,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1972687542438507, 2: 0.1534060953250289 \t Acc: 0.3843879699707031\n",
      "Train Loss :1.3301645517349243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 79/276 [04:04<11:49,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.018614472821354866, 2: 0.017907333352946538 \t Acc: 0.34737062454223633\n",
      "Train Loss :1.3499524593353271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 80/276 [04:07<10:55,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.06496443599462509, 2: 0.05231454347483829 \t Acc: 0.31687450408935547\n",
      "Train Loss :1.3599282503128052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 81/276 [04:09<10:10,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.13904736936092377, 2: 0.12731830677334688 \t Acc: 0.357269287109375\n",
      "Train Loss :1.3697751760482788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██▉       | 82/276 [04:13<10:50,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.04990481957793236, 2: 0.04268298835695721 \t Acc: 0.3666563034057617\n",
      "Train Loss :1.3349661827087402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 83/276 [04:16<10:03,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.0855380967259407, 2: 0.0889984299901306 \t Acc: 0.350341796875\n",
      "Train Loss :1.2716405391693115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 84/276 [04:18<09:32,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11483065038919449, 2: 0.14251485888760937 \t Acc: 0.34810829162597656\n",
      "Train Loss :1.1979645490646362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 85/276 [04:21<09:08,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1284124255180359, 2: 0.10857182519166812 \t Acc: 0.34471940994262695\n",
      "Train Loss :1.315753698348999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 86/276 [04:24<08:49,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15413640439510345, 2: 0.12990960570536483 \t Acc: 0.34756040573120117\n",
      "Train Loss :1.216029167175293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 87/276 [04:27<09:13,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08114365488290787, 2: 0.12177044394943227 \t Acc: 0.34295225143432617\n",
      "Train Loss :1.3360919952392578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 88/276 [04:29<08:53,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.014436135068535805, 2: 0.018767616030923685 \t Acc: 0.30279064178466797\n",
      "Train Loss :1.3247886896133423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 89/276 [04:32<08:40,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07960673421621323, 2: 0.09788816819269287 \t Acc: 0.3426494598388672\n",
      "Train Loss :1.2119464874267578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 90/276 [04:35<08:27,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.015417358838021755, 2: 0.015515766320854253 \t Acc: 0.3695340156555176\n",
      "Train Loss :1.392266035079956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 91/276 [04:38<09:24,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03638974204659462, 2: 0.05748169850883785 \t Acc: 0.3385601043701172\n",
      "Train Loss :1.1834015846252441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 92/276 [04:41<08:56,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.042360514402389526, 2: 0.048452110894530796 \t Acc: 0.3661494255065918\n",
      "Train Loss :1.490024447441101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▎      | 93/276 [04:44<08:38,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03934670612215996, 2: 0.031214250803824124 \t Acc: 0.30907392501831055\n",
      "Train Loss :1.3573824167251587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 94/276 [04:46<08:26,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.018869206309318542, 2: 0.035990337501953615 \t Acc: 0.3294086456298828\n",
      "Train Loss :1.3625404834747314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 95/276 [04:49<08:19,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03139930218458176, 2: 0.031164831074748395 \t Acc: 0.3148508071899414\n",
      "Train Loss :1.3650298118591309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▍      | 96/276 [04:52<08:09,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09503592550754547, 2: 0.12006084712160696 \t Acc: 0.32697296142578125\n",
      "Train Loss :1.2303242683410645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 97/276 [04:54<08:00,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08369988948106766, 2: 0.06502908227716313 \t Acc: 0.35767650604248047\n",
      "Train Loss :1.2312713861465454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 98/276 [04:57<07:55,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18783220648765564, 2: 0.17162646077761828 \t Acc: 0.4204578399658203\n",
      "Train Loss :1.3036059141159058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 99/276 [05:00<07:54,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.017945902422070503, 2: 0.030691730635031403 \t Acc: 0.36415767669677734\n",
      "Train Loss :1.3275119066238403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 100/276 [05:04<09:05,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.059521082788705826, 2: 0.09050920459236136 \t Acc: 0.3517613410949707\n",
      "Train Loss :1.2785351276397705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 101/276 [05:06<08:39,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03414662182331085, 2: 0.06621655112937169 \t Acc: 0.3783750534057617\n",
      "Train Loss :1.3632419109344482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 102/276 [05:09<08:19,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.0185591708868742, 2: 0.03003226397283627 \t Acc: 0.34343910217285156\n",
      "Train Loss :1.4173084497451782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 103/276 [05:12<08:34,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1704133152961731, 2: 0.17984117524191864 \t Acc: 0.5150647163391113\n",
      "Train Loss :1.2552530765533447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 104/276 [05:15<08:10,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07153069972991943, 2: 0.06964885588177697 \t Acc: 0.41101551055908203\n",
      "Train Loss :1.276153326034546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 105/276 [05:17<07:53,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.074924536049366, 2: 0.08888488894512572 \t Acc: 0.5124397277832031\n",
      "Train Loss :1.3377608060836792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 106/276 [05:21<08:29,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11115088313817978, 2: 0.11643427303426687 \t Acc: 0.5215334892272949\n",
      "Train Loss :1.3448587656021118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 107/276 [05:24<08:40,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08925552666187286, 2: 0.09703806580471723 \t Acc: 0.4990091323852539\n",
      "Train Loss :1.3589311838150024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 108/276 [05:28<09:32,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1294279247522354, 2: 0.18633991049490084 \t Acc: 0.5059237480163574\n",
      "Train Loss :1.2500264644622803\n"
     ]
    }
   ],
   "source": [
    "# Run learn from scratch (UNCOMMENT TO RUN)\n",
    "\n",
    "t_loss, t_acc, t_history, v_loss, v_acc, v_history = learn_from_scratch(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
