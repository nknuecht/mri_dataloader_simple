{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import pprint\n",
    "import tqdm\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import KFold\n",
    "# from torchinfo import summary\n",
    "\n",
    "# import cv2\n",
    "import nibabel as nib\n",
    "import skimage.transform as skTrans\n",
    "from numpy import logical_and as l_and, logical_not as l_not\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproduciablity\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleToFixed(object):\n",
    "\n",
    "    def __init__(self, new_shape, interpolation=1, channels=4):\n",
    "        self.shape= new_shape\n",
    "        self.interpolation = interpolation\n",
    "        self.channels = channels\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # print('first shape', image.shape)\n",
    "        if image is not None: # (some patients don't have segmentations)\n",
    "            if self.channels == 1:\n",
    "                short_shape = (self.shape[1], self.shape[2], self.shape[3])\n",
    "                image = skTrans.resize(image, short_shape, order=self.interpolation, preserve_range=True)  #\n",
    "                image = image.reshape(self.shape)\n",
    "            else:\n",
    "                image = skTrans.resize(image, self.shape, order=self.interpolation, preserve_range=True)  #\n",
    "\n",
    "        # print('second shape', image.shape)\n",
    "        # print()\n",
    "        return image\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \"\"\"Randomly flips (horizontally as well as vertically) the given PIL.Image with a probability of 0.5\n",
    "    \"\"\"\n",
    "    def __init__(self, prob_flip=0.5):\n",
    "        self.prob_flip= prob_flip\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if random.random() < self.prob_flip:\n",
    "            flip_type = np.random.randint(0, 3) # flip across any 3D axis\n",
    "            image = np.flip(image, flip_type)\n",
    "        return image\n",
    "\n",
    "class ZeroChannel(object):\n",
    "    \"\"\"Randomly sets channel to zero the given PIL.Image with a probability of 0.25\n",
    "    \"\"\"\n",
    "    def __init__(self, prob_zero=0.25, channels=4):\n",
    "        self.prob_zero= prob_zero\n",
    "        self.channels = channels\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if np.random.random() < self.prob_zero:\n",
    "            channel_to_zero = np.random.randint(0, self.channels) # flip across any 3D axis\n",
    "            zeros = np.zeros((image.shape[1], image.shape[2], image.shape[3]))\n",
    "            image[channel_to_zero, :, :, :] = zeros\n",
    "        return image\n",
    "\n",
    "class ZeroSprinkle(object):\n",
    "    def __init__(self, prob_zero=0.25, prob_true=0.5, channels=4):\n",
    "        self.prob_zero=prob_zero\n",
    "        self.prob_true=prob_true\n",
    "        self.channels=channels\n",
    "    def __call__(self, image):\n",
    "\n",
    "        if self.prob_true:\n",
    "            mask = np.random.rand(image.shape[0], image.shape[1], image.shape[2], image.shape[3])\n",
    "            mask[mask < self.prob_zero] = 0\n",
    "            mask[mask > 0] = 1\n",
    "            image = image*mask\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "class MinMaxNormalize(object):\n",
    "    \"\"\"Min-Max normalization\n",
    "    \"\"\"\n",
    "    def __call__(self, image):\n",
    "        def norm(im):\n",
    "            im = im.astype(np.float32)\n",
    "            min_v = np.min(im)\n",
    "            max_v = np.max(im)\n",
    "            im = (im - min_v)/(max_v - min_v)\n",
    "            return im\n",
    "        image = norm(image)\n",
    "        return image\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, scale=1):\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if image is not None:\n",
    "            image = image.astype(np.float32)\n",
    "            image = image.reshape((image.shape[0], int(image.shape[1]/self.scale), int(image.shape[2]/self.scale), int(image.shape[3]/self.scale)))\n",
    "            image_tensor = torch.from_numpy(image)\n",
    "            return image_tensor\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"\n",
    "    Composes several transforms together.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for i, t in enumerate(self.transforms):\n",
    "            image = t(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bb_3D(img, pad=0):\n",
    "    '''\n",
    "    This function returns a tumor 3D bounding box using a segmentation mask\n",
    "    '''\n",
    "    xs = np.nonzero(np.sum(np.sum(img, axis=1), axis=1))\n",
    "    ys = np.nonzero(np.sum(np.sum(img, axis=0), axis=1))\n",
    "    zs = np.nonzero(np.sum(np.sum(img, axis=0), axis=0))\n",
    "    xmin, xmax = np.min(xs), np.max(xs)\n",
    "    ymin, ymax = np.min(ys), np.max(ys)\n",
    "    zmin, zmax = np.min(zs), np.max(zs)\n",
    "    bbox = (xmin-pad, ymin-pad, zmin-pad, xmax+pad, ymax+pad, zmax+pad)\n",
    "    return bbox\n",
    "\n",
    "def min_max(img):\n",
    "    '''\n",
    "    Min-max normalization\n",
    "    '''\n",
    "    return (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "def read_mri(mr_path_dict, pad=0):\n",
    "\n",
    "    image_shape = nib.load(mr_path_dict['flair']).get_fdata().shape\n",
    "    bb_seg = get_bb_3D(nib.load(mr_path_dict['flair']).get_fdata())\n",
    "    (xmin, ymin, zmin, xmax, ymax, zmax) = bb_seg\n",
    "\n",
    "    xmin = np.max([0, xmin-pad])\n",
    "    ymin = np.max([0, ymin-pad])\n",
    "    zmin = np.max([0, zmin-pad])\n",
    "\n",
    "    xmax = np.min([image_shape[0]-1, xmax+pad])\n",
    "    ymax = np.min([image_shape[1]-1, ymax+pad])\n",
    "    zmax = np.min([image_shape[2]-1, zmax+pad])\n",
    "\n",
    "\n",
    "    img_dict = {}\n",
    "    for key in ['flair', 't1', 't1ce', 't2', 'seg']:\n",
    "        img = nib.load(mr_path_dict[key])\n",
    "        img_data = img.get_fdata()\n",
    "        img_dict[key] = img_data[xmin:xmax, ymin:ymax, zmin:zmax]\n",
    "\n",
    "    stacked_img = np.stack([min_max(img_dict['flair']), min_max(img_dict['t1']),min_max(img_dict['t1ce']),min_max(img_dict['t2'])], axis=0)\n",
    "    return stacked_img, img_dict['seg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_(image, seg, predicted=False):\n",
    "    #Overlay with Predicted\n",
    "    img = image[slice, :, :, :].squeeze()\n",
    "    img = utils.make_grid(img)\n",
    "    img = img.detach().cpu().numpy()\n",
    "    \n",
    "    print(img.shape)\n",
    "    \n",
    "    # plot images\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    img_list = [img[i].T for i in range(channels)] # 1 image per channel\n",
    "    plt.imshow(np.hstack(img_list), cmap='Greys_r')\n",
    "    \n",
    "    ## plot segmentation mask ##\n",
    "    seg_img = torch.tensor(pred[slice].squeeze())\n",
    "    if not predicted:\n",
    "        seg_img = torch.tensor(seg_img.numpy()[:, ::-1].copy()) #flip\n",
    "    seg_img = utils.make_grid(seg_img).detach().cpu().numpy()\n",
    "    \n",
    "    print(np.unique(seg_img))\n",
    "\n",
    "    plt.imshow(np.hstack([seg_img[0].T]), cmap='Greys_r', alpha=0.3)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                metadata_df,\n",
    "                root_dir,\n",
    "                transform=None,\n",
    "                seg_transform=None, ###\n",
    "                dataformat=None, # indicates what shape (or content) should be returned (2D or 3D, etc.)\n",
    "                returndims=None, # what size/shape 3D volumes should be returned as.\n",
    "                output_shape=None,\n",
    "                visualize=False,\n",
    "                modality=None,\n",
    "                pad=2,\n",
    "                device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metadata_df (string): Path to the csv file w/ patient IDs\n",
    "            root_dir (string): Directory for MR images\n",
    "            transform (callable, optional)\n",
    "        \"\"\"\n",
    "        self.device=device\n",
    "        self.metadata_df = metadata_df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.seg_transform = seg_transform\n",
    "        self.returndims=returndims\n",
    "        self.modality = modality\n",
    "        self.pad = pad\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(type(idx), idx)\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        BraTS20ID = self.metadata_df.iloc[idx].BraTS_2020_subject_ID\n",
    "\n",
    "        # make dictonary of paths to MRI volumnes (modalities) and segmenation masks\n",
    "        mr_path_dict = {}\n",
    "        sequence_type = ['seg', 't1', 't1ce', 'flair', 't2']\n",
    "        for seq in sequence_type:\n",
    "            mr_path_dict[seq] = os.path.join(self.root_dir, BraTS20ID, BraTS20ID + '_'+seq+'.nii.gz')\n",
    "\n",
    "        image, seg_image = read_mri(mr_path_dict=mr_path_dict, pad=self.pad)\n",
    "        \n",
    "        if seg_image is not None:\n",
    "            if self.output_shape == 'wt':\n",
    "                seg_image[np.nonzero(seg_image)] = 1 #only 0's and 1's for background and tumor\n",
    "            else:\n",
    "                seg_image[seg_image == 4] = 3 #0,1,2,3 for background and tumor regions\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.seg_transform:\n",
    "            seg_image = self.seg_transform(seg_image)\n",
    "        else:\n",
    "            print('no transform')\n",
    "        # print(image.shape)\n",
    "        return (image, seg_image), BraTS20ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(params):\n",
    "    if platform.system() == 'Windows':\n",
    "        naming = pd.read_csv(f\"{params['image_dir']}\\\\name_mapping.csv\")\n",
    "    else:\n",
    "        naming = pd.read_csv(f\"{params['image_dir']}/name_mapping.csv\")\n",
    "    \n",
    "    data_df = pd.DataFrame(naming['BraTS_2020_subject_ID'])\n",
    "\n",
    "    # n_patients_to_train_with\n",
    "    total_num_patients = len(data_df)\n",
    "\n",
    "    assert sum(params['tr_va_te_split']) == 100\n",
    "    tr_split = int((total_num_patients * params['tr_va_te_split'][0]) / 100)\n",
    "    va_split = int((total_num_patients * params['tr_va_te_split'][1]) / 100)\n",
    "    te_split = total_num_patients - (tr_split + va_split)\n",
    "\n",
    "    print(f\"Data is split into train: {tr_split}, validation: {va_split} and test: {te_split}\")\n",
    "                   \n",
    "    train_df = data_df[: tr_split]\n",
    "    valid_df = data_df[tr_split : (tr_split + va_split)]\n",
    "    test_df = data_df[(tr_split + va_split) :]\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dataset(df, train=False):\n",
    "\n",
    "    image_dir, channels, resize_shape, output_shape = params['image_dir'], \\\n",
    "                                                      params['channels'], \\\n",
    "                                                      params['resize_shape'], \\\n",
    "                                                      params['output_shape']\n",
    "\n",
    "    # basic data augmentation\n",
    "    prob_voxel_zero = 0 # 0.1\n",
    "    prob_channel_zero = 0 # 0.5\n",
    "    prob_true = 0 # 0.8\n",
    "    randomflip = RandomFlip()\n",
    "\n",
    "    # MRI transformations\n",
    "    train_transformations = Compose([\n",
    "        MinMaxNormalize(),\n",
    "        ScaleToFixed((channels, resize_shape[0], resize_shape[1],\n",
    "                      resize_shape[2]), interpolation=1, channels=channels),\n",
    "        ZeroSprinkle(prob_zero=prob_voxel_zero, prob_true=prob_true),\n",
    "        ZeroChannel(prob_zero=prob_channel_zero),\n",
    "        randomflip,\n",
    "        ToTensor()\n",
    "        ])\n",
    "    \n",
    "    val_transformations = Compose([\n",
    "            MinMaxNormalize(),\n",
    "            ScaleToFixed((channels, resize_shape[0], resize_shape[1],\n",
    "                          resize_shape[2]), interpolation=1, channels=channels),\n",
    "            ToTensor(),\n",
    "        ])\n",
    "\n",
    "    # GT segmentation mask transformations\n",
    "    seg_transformations = Compose([\n",
    "        ScaleToFixed((1, resize_shape[0], resize_shape[1],\n",
    "                      resize_shape[2]), interpolation=0, channels=1),\n",
    "        randomflip,\n",
    "        ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    if train:\n",
    "        dataset = GeneralDataset(metadata_df=df, \n",
    "                                root_dir=image_dir,\n",
    "                                transform=train_transformations,\n",
    "                                seg_transform=seg_transformations,\n",
    "                                returndims=resize_shape,\n",
    "                                output_shape=output_shape)\n",
    "    else:\n",
    "        dataset = GeneralDataset(metadata_df=df, \n",
    "                                root_dir=image_dir,\n",
    "                                transform=val_transformations,\n",
    "                                seg_transform=seg_transformations,\n",
    "                                returndims=resize_shape,\n",
    "                                output_shape=output_shape)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(params):\n",
    "\n",
    "    train_df, valid_df, test_df = read_dataframe(params)\n",
    "\n",
    "    train_dataset = retrieve_dataset(train_df, train=True)\n",
    "    \n",
    "    valid_dataset = retrieve_dataset(valid_df)\n",
    "\n",
    "    test_dataset = retrieve_dataset(test_df)\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['train_batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=params['train_batch_size'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['test_batch_size'])\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, norm='b', num_groups=2, k_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv3d = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=k_size,\n",
    "                                stride=stride, padding=padding)\n",
    "        if norm == 'b':\n",
    "            self.norm = nn.BatchNorm3d(num_features=out_channels)\n",
    "        else:\n",
    "            # use only one group if the given number of groups is greater than the number of channels\n",
    "            if out_channels < num_groups:\n",
    "                num_groups = 1\n",
    "            assert out_channels % num_groups == 0, f'Expected out_channels{out_channels} in input to be divisible by num_groups{num_groups}'\n",
    "            self.norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(self.conv3d(x))\n",
    "        x = F.elu(x) #!\n",
    "        return x\n",
    "\n",
    "class ConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k_size=3, stride=2, padding=1, output_padding=1):\n",
    "        super(ConvTranspose, self).__init__()\n",
    "        self.conv3d_transpose = nn.ConvTranspose3d(in_channels=in_channels,\n",
    "                                                   out_channels=out_channels,\n",
    "                                                   kernel_size=k_size,\n",
    "                                                   stride=stride,\n",
    "                                                   padding=padding,\n",
    "                                                   output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv3d_transpose(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, init_features, norm='b', num_groups=2, model_depth=4, pool_size=2):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.root_feat_maps = init_features\n",
    "        self.num_conv_blocks = 2\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "        for depth in range(model_depth):\n",
    "            feat_map_channels = 2 ** (depth + 1) * self.root_feat_maps\n",
    "            for i in range(self.num_conv_blocks):\n",
    "                self.conv_block = ConvBlock(in_channels=in_channels, out_channels=feat_map_channels, norm=norm, num_groups=num_groups)\n",
    "                self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv_block\n",
    "                in_channels, feat_map_channels = feat_map_channels, feat_map_channels * 2\n",
    "            if depth == model_depth - 1:\n",
    "                break\n",
    "            else:\n",
    "                self.pooling = nn.MaxPool3d(kernel_size=pool_size, stride=2, padding=0)\n",
    "                self.module_dict[\"max_pooling_{}\".format(depth)] = self.pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_sampling_features = []\n",
    "        for k, op in self.module_dict.items():\n",
    "            if k.startswith(\"conv\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "                if k.endswith(\"1\"):\n",
    "                    down_sampling_features.append(x)\n",
    "            elif k.startswith(\"max_pooling\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "\n",
    "        return x, down_sampling_features\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels, init_features, norm, num_groups=2, model_depth=4):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.num_conv_blocks = 2\n",
    "        self.num_feat_maps = init_features\n",
    "        # user nn.ModuleDict() to store ops\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "\n",
    "        for depth in range(model_depth - 2, -1, -1):\n",
    "            # print(depth)\n",
    "            feat_map_channels = 2 ** (depth + 1) * self.num_feat_maps\n",
    "            # print(feat_map_channels * 4)\n",
    "            self.deconv = ConvTranspose(in_channels=feat_map_channels * 4, out_channels=feat_map_channels * 4)\n",
    "            self.module_dict[\"deconv_{}\".format(depth)] = self.deconv\n",
    "            for i in range(self.num_conv_blocks):\n",
    "                if i == 0:\n",
    "                    self.conv = ConvBlock(in_channels=feat_map_channels * 6, out_channels=feat_map_channels * 2, norm=norm, num_groups=num_groups)\n",
    "                    self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv\n",
    "                else:\n",
    "                    self.conv = ConvBlock(in_channels=feat_map_channels * 2, out_channels=feat_map_channels * 2, norm=norm, num_groups=num_groups)\n",
    "                    self.module_dict[\"conv_{}_{}\".format(depth, i)] = self.conv\n",
    "            if depth == 0:\n",
    "                self.final_conv = ConvBlock(in_channels=feat_map_channels * 2, out_channels=out_channels, norm=norm, num_groups=num_groups)\n",
    "                self.module_dict[\"final_conv\"] = self.final_conv\n",
    "\n",
    "    def forward(self, x, down_sampling_features):\n",
    "        \"\"\"\n",
    "        :param x: inputs\n",
    "        :param down_sampling_features: feature maps from encoder path\n",
    "        :return: output\n",
    "        \"\"\"\n",
    "        for k, op in self.module_dict.items():\n",
    "            if k.startswith(\"deconv\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "                x = torch.cat((down_sampling_features[int(k[-1])], x), dim=1)\n",
    "            elif k.startswith(\"conv\"):\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "            else:\n",
    "                x = op(x)\n",
    "                #print(k, x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, init_features, norm, num_groups=2, model_depth=4, final_activation=\"sigmoid\"):\n",
    "        super(UnetModel, self).__init__()\n",
    "        self.encoder = EncoderBlock(in_channels=in_channels,\n",
    "                                    init_features=init_features,\n",
    "                                    norm=norm, num_groups=num_groups,\n",
    "                                    model_depth=model_depth)\n",
    "        self.decoder = DecoderBlock(out_channels=out_channels,\n",
    "                                    init_features=init_features,\n",
    "                                    norm=norm, num_groups=num_groups,\n",
    "                                    model_depth=model_depth)\n",
    "        if final_activation == \"sigmoid\":\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        else:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, downsampling_features = self.encoder(x)\n",
    "        x = self.decoder(x, downsampling_features)\n",
    "        x = self.sigmoid(x)\n",
    "        # print(\"Final output shape: \", x.shape)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        # smooth factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        batch_size = targets.size(0)\n",
    "        # log_prob = torch.sigmoid(logits)\n",
    "        logits = logits.view(batch_size, -1).type(torch.FloatTensor)\n",
    "        targets = targets.view(batch_size, -1).type(torch.FloatTensor)\n",
    "        intersection = (logits * targets).sum(-1)\n",
    "        dice_score = 2. * intersection / ((logits + targets).sum(-1) + self.epsilon)\n",
    "        # dice_score = 1 - dice_score.sum() / batch_size\n",
    "        dice_score = torch.mean(1. - dice_score)\n",
    "        dice_score.requires_grad = True\n",
    "        return dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss tailored to Brats need.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, do_sigmoid=True):\n",
    "        super(EDiceLoss, self).__init__()\n",
    "        self.do_sigmoid = do_sigmoid\n",
    "        self.labels = [\"ET\", \"TC\", \"WT\"]\n",
    "        self.device = \"cpu\"\n",
    "\n",
    "    def binary_dice(self, inputs, targets, label_index, metric_mode=False):\n",
    "        smooth = 1.\n",
    "        if self.do_sigmoid:\n",
    "            inputs = torch.sigmoid(inputs)\n",
    "\n",
    "        if metric_mode:\n",
    "            inputs = inputs > 0.5\n",
    "            if targets.sum() == 0:\n",
    "                print(f\"No {self.labels[label_index]} for this patient\")\n",
    "                if inputs.sum() == 0:\n",
    "                    return torch.tensor(1., device=device)\n",
    "                else:\n",
    "                    return torch.tensor(0., device=device)\n",
    "            # Threshold the pred\n",
    "        intersection = EDiceLoss.compute_intersection(inputs, targets)\n",
    "        if metric_mode:\n",
    "            dice = (2 * intersection) / ((inputs.sum() + targets.sum()) * 1.0)\n",
    "        else:\n",
    "            dice = (2 * intersection + smooth) / (inputs.pow(2).sum() + targets.pow(2).sum() + smooth)\n",
    "        if metric_mode:\n",
    "            return dice\n",
    "        return 1 - dice\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_intersection(inputs, targets):\n",
    "        intersection = torch.sum(inputs * targets)\n",
    "        return intersection\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        dice = 0\n",
    "        for i in range(target.size(1)):\n",
    "            dice = dice + self.binary_dice(inputs[:, i, ...], target[:, i, ...], i)\n",
    "        final_dice = dice / target.size(1)\n",
    "        final_dice.requires_grad = True\n",
    "        return final_dice\n",
    "\n",
    "    def metric(self, inputs, target):\n",
    "        dices = []\n",
    "        for j in range(target.size(0)):\n",
    "            dice = []\n",
    "            for i in range(target.size(1)):\n",
    "                dice.append(self.binary_dice(inputs[j, i], target[j, i], i, True))\n",
    "            dices.append(dice)\n",
    "        return dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, targets, patient):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds:\n",
    "        torch tensor of size 1*C*Z*Y*X, ours BS*Z*Y*X \n",
    "    targets:\n",
    "        torch tensor of same shape\n",
    "    patient :\n",
    "        The patient ID\n",
    "    \"\"\"\n",
    "\n",
    "    assert preds.shape == targets.shape, \"Preds and targets do not have the same size\"\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    \n",
    "    preds, targets = preds.detach().cpu().numpy(), targets.detach().cpu().numpy()\n",
    "\n",
    "    metrics_list = []\n",
    "\n",
    "    metrics = dict(\n",
    "        patient_id=patient,\n",
    "    )\n",
    "    # print(targets.shape, targets.dtype, targets)\n",
    "    \n",
    "    if np.sum(targets) == 0:\n",
    "        print(f\"{label} not present for {patient}\")\n",
    "    else:\n",
    "        tp = np.sum(l_and(preds, targets))\n",
    "        tn = np.sum(l_and(l_not(preds), l_not(targets)))\n",
    "        fp = np.sum(l_and(preds, l_not(targets)))\n",
    "        fn = np.sum(l_and(l_not(preds), targets))\n",
    "\n",
    "        sens = tp / (tp + fn)\n",
    "        spec = tn / (tn + fp)\n",
    "        acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "        dice = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "    metrics[DICE] = dice\n",
    "    metrics[ACC] = acc\n",
    "    metrics[SENS] = sens\n",
    "    metrics[SPEC] = spec\n",
    "    # pp.pprint(metrics)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "    return acc, dice, metrics_list\n",
    "\n",
    "\n",
    "DICE = \"dice\"\n",
    "ACC = \"acc\"\n",
    "SENS = \"sens\"\n",
    "SPEC = \"spec\"\n",
    "METRICS = [DICE, ACC, SENS, SPEC]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dice(preds, targets):\n",
    "    return (2 * torch.sum(preds * targets)) / ((preds.sum() + preds.sum()) * 1.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_metric(train, label, metric_name):\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.semilogy(train, label=label)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'Model {metric_name} Plot')\n",
    "    plt.savefig(f'Model_{metric_name}_{label}_Plot.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def plot_result(kfolds, num_epochs, fold_train_history, fold_valid_history):\n",
    "    final_fold = {'train_loss':[],'valid_loss':[],'train_acc':[],'valid_acc':[]}\n",
    "\n",
    "    for epoch in range(num_epochs):                                      \n",
    "        final_fold['train_loss'].append(np.mean([fold_train_history[str(fold)]['train_loss'][epoch] for fold in range(kfolds)]))\n",
    "        final_fold['train_acc'].append(np.mean([fold_train_history[str(fold)]['train_acc'][epoch]for fold in range(kfolds)]))\n",
    "\n",
    "    plot_metric(final_fold['train_loss'], 'train', 'Loss')\n",
    "    plot_metric(final_fold['train_acc'], 'validation', 'Accuracy')\n",
    "\n",
    "    final_fold['valid_loss'].append([fold_valid_history[str(fold)]['valid_loss'] for fold in range(kfolds)])\n",
    "    final_fold['valid_acc'].append([fold_valid_history[str(fold)]['valid_acc'] for fold in range(kfolds)])\n",
    "\n",
    "    print(final_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, params):\n",
    "\n",
    "    test_model = UnetModel(params['pretrain_in_channels'],\n",
    "                           params['pretrain_out_channels'],\n",
    "                           params['init_features'], params['norm'],\n",
    "                           params['num_groups'],\n",
    "                           )\n",
    "\n",
    "    test_optimizer = torch.optim.AdamW(test_model.parameters(),\n",
    "                                       lr=params['learning_rate'],\n",
    "                                       )\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(path)\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "    test_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    test_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    return test_model, test_optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_folds(model, optimizer, fold, epoch, loss):\n",
    "    # Saving the model\n",
    "    save_path = f'model-fold-{fold}.pth'\n",
    "\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': loss,\n",
    "                  }\n",
    "    torch.save(checkpoint, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_nofolds(model, optimizer, epoch, loss, params):\n",
    "    now = datetime.now()\n",
    "    # dd/mm/YY H:M:S\n",
    "    dt_string = now.strftime(\"%d_%H_%M\")\n",
    "\n",
    "    # Saving the model\n",
    "    save_path = f\"model_{params['output_shape']}_{params['run_name']}_{dt_string}.pth\"\n",
    "\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': loss,\n",
    "                  'params': params,\n",
    "                  }\n",
    "    torch.save(checkpoint, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldRunAll(criterion, dataset, params): \n",
    "                \n",
    "    k_folds, num_epochs, train_batch_size = params['k_folds'],\\\n",
    "                                            params['no_epochs'],\\\n",
    "                                            params['train_batch_size']\n",
    "    \n",
    "    use_cuda, loss_name, in_channels, out_channels = params['use_cuda'], \\\n",
    "                                                     params['loss_name'],\\\n",
    "                                                     params['in_channels'],\\\n",
    "                                                     params['out_channels'],\n",
    "\n",
    "    init_features, learning_rate, norm, num_groups = params['init_features'],\\\n",
    "                                                     params['learning_rate'],\\\n",
    "                                                     params['norm'], \\\n",
    "                                                     params['num_groups'],\n",
    "\n",
    "    loss_function = criterion\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    fold_train_history = {}\n",
    "    fold_valid_history = {}\n",
    "    fold_train_and_valid_acc = {}\n",
    "    fold_train_and_valid_loss = {}\n",
    "\n",
    "    print('--------------------------------')\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        valid_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        dataloader_train = DataLoader(dataset, batch_size=train_batch_size, sampler=train_subsampler, num_workers=0)\n",
    "        dataloader_valid = DataLoader(dataset, batch_size=train_batch_size, sampler=valid_subsampler, num_workers=0)\n",
    "\n",
    "        # Initialize optimizer and Model\n",
    "        model = UnetModel(in_channels=in_channels, out_channels=out_channels,\n",
    "                          init_features=init_features, norm=norm, num_groups=num_groups)\n",
    "        if use_cuda:\n",
    "            model = model.cuda()\n",
    "        #print(model)\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Run the training, testing and saving loop for defined number of epochs\n",
    "        start_time = time.time()\n",
    "\n",
    "        t_loss, t_acc, t_history, v_loss, v_acc, v_history = train_test_folds(model, loss_function, optimizer,\n",
    "                                                                               dataloader_train, dataloader_valid,\n",
    "                                                                               fold, num_epochs, use_cuda,\n",
    "                                                                               loss_name)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Epoch Time: {end_time - start_time}\")\n",
    "\n",
    "        #Saving loss results \n",
    "        fold_train_and_valid_loss[str(fold)] = [t_loss, v_loss]\n",
    "        fold_train_and_valid_acc[str(fold)] = [t_acc, v_acc]\n",
    "        fold_train_history[str(fold)] = t_history\n",
    "        fold_valid_history[str(fold)] = v_history\n",
    "\n",
    "        # Print accuracy\n",
    "        print(f'Accuracy for fold {fold}: {v_acc}')\n",
    "        print(f'Loss for fold {fold}: {v_loss}')\n",
    "        print('--------------------------------')  \n",
    "\n",
    "    return fold_train_history, fold_valid_history, fold_train_and_valid_loss, fold_train_and_valid_acc\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_folds(model, loss_function, optimizer, dataloader_train, dataloader_valid, fold, num_epochs, use_cuda, loss_name):\n",
    "    train_history = {'train_loss': [], 'train_acc':[], 'train_dice':[]}\n",
    "    valid_history = {'valid_loss': [], 'valid_acc':[], 'valid_dice':[]}\n",
    "    best = math.inf\n",
    "\n",
    "    edice = EDiceLoss()\n",
    "    if use_cuda:\n",
    "        edice = edice.cuda()\n",
    "    metric = edice.metric\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Starting Train epoch: {epoch+1}')\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc, train_dice = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader_train)):\n",
    "            (inputs, targets), ID = data\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape, outputs.shape, targets.squeeze(1).long().shape)\n",
    "\n",
    "            if loss_name == 'dice':\n",
    "                class_outputs = outputs.argmax(dim=1)\n",
    "                loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "            else:\n",
    "                loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "            train_loss += loss.item() * outputs.size(0) #multiplying by batchsize\n",
    "            \n",
    "            rtrain_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "            rtrain_acc, rtrain_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "            print(f'Train Dice 1: {rtrain_dice1}, 2: {rtrain_dice2} \\t Acc: {rtrain_acc}')\n",
    "            train_acc += rtrain_acc\n",
    "            train_dice += rtrain_dice1\n",
    "            \n",
    "            print(f'Train Loss :{loss.item()}')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "        train_history['train_loss'].append(train_loss / len(dataloader_train.sampler))\n",
    "        train_history['train_acc'].append(train_acc / len(dataloader_train.sampler))\n",
    "        train_history['train_dice'].append(train_dice / len(dataloader_train.sampler))\n",
    "\n",
    "        print(f\"Train Epoch loss: {train_history['train_loss'][-1]}, \\t ACC/DICE :{train_history['train_acc'][-1]}/{train_history['train_dice'][-1]} \")\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    valid_acc, valid_dice = 0, 0\n",
    "           \n",
    "    model.eval()\n",
    "    #! maybe change later to validate after some epochs\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test data and generate predictions\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader_valid)):\n",
    "            (inputs, targets), ID = data\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda() \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if params['loss_name'] == 'dice':\n",
    "                class_outputs = outputs.argmax(dim=1)\n",
    "                loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "            else:\n",
    "                loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "            # print('Valid Loss:', loss.item())\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            rvalid_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "\n",
    "            rvalid_acc, rvalid_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "            # print(f'Val Dice 1: {rvalid_dice1}, 2: {rvalid_dice2}')\n",
    "            valid_acc += rvalid_acc\n",
    "            valid_dice += rvalid_dice1\n",
    "            \n",
    "        # Print accuracy\n",
    "        print(f'Val Dice : {valid_dice}, len {len(dataloader_valid.sampler)}')\n",
    "        valid_loss /= len(dataloader_valid.sampler) \n",
    "        valid_acc = valid_acc / len(dataloader_valid.sampler)\n",
    "        valid_dice = valid_dice / len(dataloader_valid.sampler)\n",
    "        #print(f\" Fold Accuracy: {valid_acc}\")\n",
    "\n",
    "    valid_history['valid_loss'].append(valid_loss)\n",
    "    valid_history['valid_acc'].append(valid_acc)\n",
    "    valid_history['valid_dice'].append(valid_dice)\n",
    "\n",
    "    print(f\"Val Epoch loss: {valid_history['valid_loss'][-1]} \\t acc/dice:/ {valid_history['valid_acc'][-1]}/ {valid_history['valid_dice'][-1]}\")\n",
    "\n",
    "    # saving best model for this fold\n",
    "    if valid_loss < best:\n",
    "        best = valid_loss\n",
    "        save_model_folds(model, optimizer, fold, epoch, loss)\n",
    "    \n",
    "    \n",
    "    return train_history['train_loss'][-1], train_history['train_acc'][-1], train_history, valid_history['valid_loss'][-1], valid_history['valid_acc'][-1], valid_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_nofolds(model, loss_function, optimizer, dataloader_train, dataloader_valid, params):\n",
    "    train_history = {'train_loss': [], 'train_acc':[], 'train_dice':[]}\n",
    "    valid_history = {'valid_loss': [], 'valid_acc':[], 'valid_dice':[]}\n",
    "    best = math.inf\n",
    "\n",
    "    edice = EDiceLoss()\n",
    "    if params['use_cuda']:\n",
    "        edice = edice.cuda()\n",
    "    metric = edice.metric\n",
    "\n",
    "    #Automatic mixed precision addition\n",
    "    #scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "    for epoch in range(params['no_epochs']):\n",
    "        print(f'Starting Train epoch: {epoch+1}')\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc, train_dice = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(tqdm.tqdm(dataloader_train)):\n",
    "            (inputs, targets), ID = data\n",
    "            if params['use_cuda']:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #with torch.cuda.amp.autocast(enabled=params['autocast']):\n",
    "            outputs = model(inputs)\n",
    "            #print(inputs.shape, outputs.shape, targets.squeeze(1).long().shape)\n",
    "\n",
    "            if params['loss_name'] == 'dice':\n",
    "                class_outputs = outputs.argmax(dim=1)\n",
    "                loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "            else:\n",
    "                loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "            train_loss += loss.item() * outputs.size(0) # multiplying by batchsize\n",
    "            \n",
    "            rtrain_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "            rtrain_acc, rtrain_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f'Train Dice 1: {rtrain_dice1}, 2: {rtrain_dice2} \\t Acc: {rtrain_acc}')\n",
    "\n",
    "            train_acc += rtrain_acc\n",
    "            train_dice += rtrain_dice1\n",
    "            if i % 10 == 0:\n",
    "                print(f'Train Loss :{loss.item()}')\n",
    "            \n",
    "            #scaler.scale(loss).backward()\n",
    "            #scaler.step(optimizer)\n",
    "            #scaler.update()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "        train_history['train_loss'].append(train_loss / len(dataloader_train.sampler))\n",
    "        train_history['train_acc'].append(train_acc / len(dataloader_train.sampler))\n",
    "        train_history['train_dice'].append(train_dice / len(dataloader_train.sampler))\n",
    "\n",
    "        print(f\"Train Epoch loss: {train_history['train_loss'][-1]}, \\t ACC/DICE :{train_history['train_acc'][-1]}/{train_history['train_dice'][-1]} \")\n",
    "          \n",
    "        if epoch % (.1 * params['no_epochs']) == 0:\n",
    "            valid_loss = 0.0\n",
    "            valid_acc, valid_dice = 0, 0\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Iterate over the test data and generate predictions\n",
    "                for i, data in enumerate(tqdm.tqdm(dataloader_valid)):\n",
    "                    (inputs, targets), ID = data\n",
    "                    if params['use_cuda']:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda() \n",
    "                      \n",
    "                    #with torch.cuda.amp.autocast(enabled=params['autocast']):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    if params['loss_name'] == 'dice':\n",
    "                        class_outputs = outputs.argmax(dim=1)\n",
    "                        loss = loss_function(class_outputs, targets.squeeze(1).long())\n",
    "                        # print(np.unique(class_outputs.detach().numpy()), class_outputs.shape, targets.squeeze(1).shape)\n",
    "                    else:\n",
    "                        loss = loss_function(outputs, targets.squeeze(1).long())\n",
    "\n",
    "                    # print('Valid Loss:', loss.item())\n",
    "                    valid_loss += loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    rvalid_dice1 = calc_dice(outputs.argmax(dim=1), targets.squeeze(1))\n",
    "\n",
    "                    rvalid_acc, rvalid_dice2, _ = calculate_metrics(outputs.argmax(dim=1), targets.squeeze(1), ID)\n",
    "                    # print(f'Val Dice 1: {rvalid_dice1}, 2: {rvalid_dice2}')\n",
    "                    valid_acc += rvalid_acc\n",
    "                    valid_dice += rvalid_dice1\n",
    "                    \n",
    "                # Print accuracy\n",
    "                print(f'Val Dice : {valid_dice}, len {len(dataloader_valid.sampler)}')\n",
    "                valid_loss /= len(dataloader_valid.sampler) \n",
    "                valid_acc = valid_acc / len(dataloader_valid.sampler)\n",
    "                valid_dice = valid_dice / len(dataloader_valid.sampler)\n",
    "\n",
    "            valid_history['valid_loss'].append(valid_loss)\n",
    "            valid_history['valid_acc'].append(valid_acc)\n",
    "            valid_history['valid_dice'].append(valid_dice)\n",
    "\n",
    "            print(f\"Val Epoch loss: {valid_history['valid_loss'][-1]} \\t acc/dice:/ {valid_history['valid_acc'][-1]}/ {valid_history['valid_dice'][-1]}\")\n",
    "\n",
    "            # saving best model for this fold\n",
    "            if valid_loss < best:\n",
    "                best = valid_loss\n",
    "                save_model_nofolds(model, optimizer, epoch, loss, params)\n",
    "    \n",
    "    \n",
    "    return train_history['train_loss'][-1], train_history['train_acc'][-1], train_history, valid_history['valid_loss'][-1], valid_history['valid_acc'][-1], valid_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfolds(params):\n",
    "    \n",
    "    if params['loss_name'] == 'ce':\n",
    "        criterion = CrossEntropyLoss()\n",
    "    elif params['loss_name'] == 'wce':\n",
    "        wisdom_weights = [1, 355.36116969, 74.37872817, 254.58104099]\n",
    "        nick_weights = [ 1.        ,  8.9263424 ,  7.79622053, 31.17438108]\n",
    "        criterion = CrossEntropyLoss(weight=torch.Tensor(nick_weights))\n",
    "    else:\n",
    "        criterion = EDiceLoss().cuda()\n",
    "      \n",
    "    if params['use_cuda']:\n",
    "        criterion.cuda()\n",
    "    \n",
    "    train_df, valid_df, _ = read_dataframe(params)\n",
    "\n",
    "    train_valid_df = pd.concat([train_df, valid_df])\n",
    "\n",
    "    train_valid_dataset = retrieve_dataset(train_valid_df)\n",
    "\n",
    "    t_history, v_history, tv_loss, tv_acc = kFoldRunAll(criterion, \n",
    "                                                        train_valid_dataset,\n",
    "                                                        params)\n",
    "    \n",
    "    plot_result(k_folds, no_epochs, t_history, v_history)\n",
    "\n",
    "    return t_history, v_history, tv_loss, tv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(path, params):\n",
    "    \n",
    "    if params['loss_name'] == 'ce':\n",
    "        criterion = CrossEntropyLoss()\n",
    "    elif params['loss_name'] == 'wce':\n",
    "        criterion = CrossEntropyLoss(weight=torch.Tensor([1, 355.36116969, 74.37872817, 254.58104099]))\n",
    "    else:\n",
    "        criterion = EDiceLoss().cuda()\n",
    "    \n",
    "    model, optimizer, _, __ = load_checkpoint(path, params)\n",
    "\n",
    "    # Transfer by changing(replacing) only last layer and finetuning to outdim=4\n",
    "    model.decoder.final_conv = ConvBlock(in_channels=params['pretrain_in_final_conv'], \n",
    "                                          out_channels=params['out_channels'],\n",
    "                                          norm=params['norm'],\n",
    "                                          num_groups=params['num_groups'])\n",
    "\n",
    "    model.decoder.module_dict.final_conv = ConvBlock(in_channels=params['pretrain_in_final_conv'], \n",
    "                                          out_channels=params['out_channels'],\n",
    "                                          norm=params['norm'],\n",
    "                                          num_groups=params['num_groups'])\n",
    "\n",
    "    if params['use_cuda']:\n",
    "        model.cuda()\n",
    "        criterion.cuda()\n",
    "    \n",
    "    dataloader_train, dataloader_valid, _ = get_data(params)\n",
    "    \n",
    "    t_loss, t_acc, t_history, v_loss, v_acc, v_history = train_test_nofolds(\n",
    "                                                model, criterion, optimizer,\n",
    "                                                dataloader_train, dataloader_valid,\n",
    "                                                params\n",
    "                                                )\n",
    "\n",
    "    # plot_result(k_folds, params['no_epochs'], t_history, v_history)\n",
    "\n",
    "    return t_loss, t_acc, t_history, v_loss, v_acc, v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_from_scratch(params):\n",
    "    \n",
    "    if params['loss_name'] == 'ce':\n",
    "        criterion = CrossEntropyLoss()\n",
    "    elif params['loss_name'] == 'wce':\n",
    "        criterion = CrossEntropyLoss(weight=torch.Tensor([1, 355.36116969, 74.37872817, 254.58104099]))\n",
    "    else:\n",
    "        criterion = EDiceLoss()\n",
    "    \n",
    "\n",
    "\n",
    "    model = UnetModel(params['in_channels'],\n",
    "                      params['out_channels'],\n",
    "                      params['init_features'],\n",
    "                      params['norm'],\n",
    "                      params['num_groups'],\n",
    "                      )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=params['learning_rate'],\n",
    "                                  )\n",
    "\n",
    "    if params['use_cuda']:\n",
    "        model.cuda()\n",
    "        criterion.cuda()\n",
    "    \n",
    "    dataloader_train, dataloader_valid, _ = get_data(params)\n",
    "    \n",
    "    t_loss, t_acc, t_history, v_loss, v_acc, v_history = train_test_nofolds(\n",
    "                                                model, criterion, optimizer,\n",
    "                                                dataloader_train, dataloader_valid,\n",
    "                                                params\n",
    "                                                )\n",
    "\n",
    "    # plot_result(k_folds, params['no_epochs'], t_history, v_history)\n",
    "\n",
    "    return t_loss, t_acc, t_history, v_loss, v_acc, v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "\n",
    "    if platform.system() == 'Windows':\n",
    "        image_dir = r\"C:\\Users\\wisdomik\\Documents\\project\\MICCAI_BraTS2020_TrainingData\"\n",
    "    else:\n",
    "        image_dir = '../../data/data/mri/MICCAI_BraTS2020_TrainingData/'\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    params = {'run_name': 'run_1',\n",
    "              'in_channels': 4,\n",
    "              'out_channels': 4,\n",
    "              'no_epochs': 20,\n",
    "              'k_folds': 5,\n",
    "              'learning_rate': 5e-4, # 1e-4,\n",
    "              'loss_name': 'wce',\n",
    "              'output_shape': 'all',\n",
    "              'tr_va_te_split': [75, 25, 0],\n",
    "              'pretrain_in_channels': 4,\n",
    "              'pretrain_out_channels': 2,\n",
    "              'pretrain_in_final_conv': 16,\n",
    "              'init_features': 8,\n",
    "              'train_batch_size': 2,\n",
    "              'autocast': False, # not in use\n",
    "              'test_batch_size': 2,\n",
    "              'norm': 'g',\n",
    "              'num_groups': 4,\n",
    "              'channels': 4,\n",
    "              'resize_shape': (128, 128, 128), # 128\n",
    "              'image_dir': image_dir,\n",
    "              'use_cuda': use_cuda\n",
    "              }\n",
    "              \n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_name': 'run_1',\n",
       " 'in_channels': 4,\n",
       " 'out_channels': 4,\n",
       " 'no_epochs': 20,\n",
       " 'k_folds': 5,\n",
       " 'learning_rate': 0.0005,\n",
       " 'loss_name': 'wce',\n",
       " 'output_shape': 'all',\n",
       " 'tr_va_te_split': [75, 25, 0],\n",
       " 'pretrain_in_channels': 4,\n",
       " 'pretrain_out_channels': 2,\n",
       " 'pretrain_in_final_conv': 16,\n",
       " 'init_features': 8,\n",
       " 'train_batch_size': 2,\n",
       " 'autocast': False,\n",
       " 'test_batch_size': 2,\n",
       " 'norm': 'g',\n",
       " 'num_groups': 4,\n",
       " 'channels': 4,\n",
       " 'resize_shape': (128, 128, 128),\n",
       " 'image_dir': '../../data/data/mri/MICCAI_BraTS2020_TrainingData/',\n",
       " 'use_cuda': True}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set params then run this\n",
    "\n",
    "#load in trained model for evaluation\n",
    "# fold_to_check = 9\n",
    "# PATH = f'drive/MyDrive/Colab Notebooks/model-fold-{fold_to_check}.pth'\n",
    "\n",
    "params = get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is split into train: 276, validation: 92 and test: 1\n",
      "Starting Train epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/138 [00:05<13:13,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.027965500950813293, 2: 0.036601789236022614 \t Acc: 0.2376103401184082\n",
      "Train Loss :1.4389846324920654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 11/138 [01:02<11:56,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.04050702601671219, 2: 0.06038346062750624 \t Acc: 0.32145023345947266\n",
      "Train Loss :1.3293557167053223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 21/138 [01:58<10:54,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18493373692035675, 2: 0.12778537623591146 \t Acc: 0.5923736095428467\n",
      "Train Loss :1.2891086339950562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 31/138 [02:54<09:58,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09025610238313675, 2: 0.09608460045612469 \t Acc: 0.5638055801391602\n",
      "Train Loss :1.2319155931472778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 41/138 [03:49<09:01,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.03735561668872833, 2: 0.04751694908438754 \t Acc: 0.4352598190307617\n",
      "Train Loss :1.318536639213562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 51/138 [04:45<08:04,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.14801514148712158, 2: 0.12452632743926106 \t Acc: 0.5982820987701416\n",
      "Train Loss :1.2624444961547852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 61/138 [05:41<07:09,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.17795802652835846, 2: 0.18862534260504574 \t Acc: 0.6014382839202881\n",
      "Train Loss :1.2770109176635742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 71/138 [06:37<06:18,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.05638434737920761, 2: 0.09278142331329214 \t Acc: 0.5844106674194336\n",
      "Train Loss :1.3778526782989502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 81/138 [07:33<05:22,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09003301709890366, 2: 0.12627907052844797 \t Acc: 0.5543906688690186\n",
      "Train Loss :1.1898796558380127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 91/138 [08:29<04:25,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11014233529567719, 2: 0.14182813875753295 \t Acc: 0.5938043594360352\n",
      "Train Loss :1.2265937328338623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 101/138 [09:26<03:29,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1704387068748474, 2: 0.17970609986755745 \t Acc: 0.6067640781402588\n",
      "Train Loss :1.2778764963150024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 111/138 [10:23<02:34,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10869162529706955, 2: 0.11182751548823493 \t Acc: 0.6039190292358398\n",
      "Train Loss :1.2077417373657227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 121/138 [11:19<01:35,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15729160606861115, 2: 0.12734926400995142 \t Acc: 0.561450719833374\n",
      "Train Loss :1.2997562885284424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 131/138 [12:16<00:39,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1492028385400772, 2: 0.1272582322924829 \t Acc: 0.5427227020263672\n",
      "Train Loss :1.3298815488815308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 138/138 [12:55<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch loss: 1.2997019135433694, \t ACC/DICE :0.271157701810201/0.05394936352968216 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [04:31<00:00,  5.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Dice : 5.305456638336182, len 92\n",
      "Val Epoch loss: 1.2244544884432917 \t acc/dice:/ 0.28661236037378723/ 0.05766800791025162\n",
      "Starting Train epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/138 [00:05<12:47,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15416760742664337, 2: 0.13787934517706943 \t Acc: 0.5847923755645752\n",
      "Train Loss :1.2094146013259888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 11/138 [01:01<11:49,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18853606283664703, 2: 0.2040498971516703 \t Acc: 0.553107738494873\n",
      "Train Loss :1.261095643043518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 21/138 [01:57<10:58,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.23464950919151306, 2: 0.19983090184313337 \t Acc: 0.582343339920044\n",
      "Train Loss :1.2184380292892456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 31/138 [02:53<10:01,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.20034009218215942, 2: 0.18030385841229682 \t Acc: 0.6181149482727051\n",
      "Train Loss :1.297532320022583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 41/138 [03:50<09:10,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16435174643993378, 2: 0.19616215304895201 \t Acc: 0.5919063091278076\n",
      "Train Loss :1.3207907676696777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 51/138 [04:46<08:06,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10702042281627655, 2: 0.12907168627142362 \t Acc: 0.4934043884277344\n",
      "Train Loss :1.230371117591858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 61/138 [05:42<07:13,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.12823837995529175, 2: 0.1845791101509137 \t Acc: 0.6080174446105957\n",
      "Train Loss :1.2173855304718018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 71/138 [06:38<06:16,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18001320958137512, 2: 0.16021456296252554 \t Acc: 0.5863511562347412\n",
      "Train Loss :1.249598741531372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 81/138 [07:34<05:17,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10586810857057571, 2: 0.11728546696922702 \t Acc: 0.6033327579498291\n",
      "Train Loss :1.3150643110275269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 91/138 [08:30<04:25,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16665150225162506, 2: 0.13409148272689525 \t Acc: 0.5943002700805664\n",
      "Train Loss :1.36073899269104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 101/138 [09:27<03:28,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09352602064609528, 2: 0.10103970188381928 \t Acc: 0.5753629207611084\n",
      "Train Loss :1.2384867668151855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 111/138 [10:22<02:31,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.13016459345817566, 2: 0.16303731862654466 \t Acc: 0.6017405986785889\n",
      "Train Loss :1.2768219709396362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 121/138 [11:19<01:34,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.25374752283096313, 2: 0.1939463957418763 \t Acc: 0.6011042594909668\n",
      "Train Loss :1.2863764762878418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 131/138 [12:15<00:39,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08818137645721436, 2: 0.07291938227691493 \t Acc: 0.5998148918151855\n",
      "Train Loss :1.3497518301010132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 138/138 [12:55<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch loss: 1.2708261928696563, \t ACC/DICE :0.29151378852733667/0.0660286620259285 \n",
      "Starting Train epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/138 [00:05<13:07,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11544197052717209, 2: 0.12266916259350051 \t Acc: 0.5946056842803955\n",
      "Train Loss :1.2454190254211426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 11/138 [01:01<11:59,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18141897022724152, 2: 0.16381531068420993 \t Acc: 0.6203203201293945\n",
      "Train Loss :1.3125232458114624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 21/138 [01:58<10:58,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.08715557307004929, 2: 0.10881839348079161 \t Acc: 0.5911655426025391\n",
      "Train Loss :1.2338769435882568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 31/138 [02:55<10:17,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15210166573524475, 2: 0.1857486079370728 \t Acc: 0.622490406036377\n",
      "Train Loss :1.225995659828186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 41/138 [03:51<09:04,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.12096279859542847, 2: 0.16534019080653387 \t Acc: 0.6406261920928955\n",
      "Train Loss :1.3226912021636963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 51/138 [04:47<08:08,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1933652013540268, 2: 0.17233420731466456 \t Acc: 0.6153395175933838\n",
      "Train Loss :1.2958273887634277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 61/138 [05:43<07:13,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.20464779436588287, 2: 0.171545176469027 \t Acc: 0.6130130290985107\n",
      "Train Loss :1.1766165494918823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 71/138 [06:42<06:41,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.23956052958965302, 2: 0.2702681753706521 \t Acc: 0.6184184551239014\n",
      "Train Loss :1.1641826629638672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 81/138 [07:43<05:45,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.07729130983352661, 2: 0.0460190888741822 \t Acc: 0.410247802734375\n",
      "Train Loss :1.2636656761169434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 91/138 [08:43<04:43,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.13821429014205933, 2: 0.1402327514546966 \t Acc: 0.6005139350891113\n",
      "Train Loss :1.2708945274353027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 101/138 [09:44<03:46,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.19787266850471497, 2: 0.19549972813633773 \t Acc: 0.6228945255279541\n",
      "Train Loss :1.2402011156082153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 111/138 [10:45<02:46,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15721194446086884, 2: 0.16780212008434364 \t Acc: 0.606393575668335\n",
      "Train Loss :1.334155797958374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 121/138 [11:46<01:43,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18922707438468933, 2: 0.18713722447062517 \t Acc: 0.6393687725067139\n",
      "Train Loss :1.2020068168640137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 131/138 [12:48<00:43,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.19079899787902832, 2: 0.18977627869063984 \t Acc: 0.6272222995758057\n",
      "Train Loss :1.263182282447815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 138/138 [13:33<00:00,  5.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch loss: 1.2679984180823616, \t ACC/DICE :0.3006081987118375/0.0671805813908577 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [05:07<00:00,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Dice : 6.455079078674316, len 92\n",
      "Val Epoch loss: 1.2433501922565957 \t acc/dice:/ 0.31518316787222156/ 0.07016390562057495\n",
      "Starting Train epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/138 [00:06<14:19,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.1563698649406433, 2: 0.14533822926933698 \t Acc: 0.6249790191650391\n",
      "Train Loss :1.3222968578338623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 11/138 [01:10<13:30,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.2108423113822937, 2: 0.21122700154366056 \t Acc: 0.6393947601318359\n",
      "Train Loss :1.288427472114563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 21/138 [02:14<12:30,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09042564779520035, 2: 0.09296361791885405 \t Acc: 0.6098966598510742\n",
      "Train Loss :1.2715884447097778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 31/138 [03:18<11:29,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.0794546902179718, 2: 0.08617175247339143 \t Acc: 0.5967156887054443\n",
      "Train Loss :1.240187406539917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 41/138 [04:21<10:10,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11991088092327118, 2: 0.15944626991707703 \t Acc: 0.6271688938140869\n",
      "Train Loss :1.2098932266235352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 51/138 [05:25<09:10,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.15128925442695618, 2: 0.16371260492690315 \t Acc: 0.6594054698944092\n",
      "Train Loss :1.2337915897369385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 61/138 [06:29<08:21,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16432584822177887, 2: 0.18393708001629996 \t Acc: 0.6347429752349854\n",
      "Train Loss :1.3032586574554443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|    | 71/138 [07:31<06:53,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.11499577015638351, 2: 0.11171049924974606 \t Acc: 0.6015560626983643\n",
      "Train Loss :1.2916491031646729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 81/138 [08:32<05:51,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.13840000331401825, 2: 0.1455679467350817 \t Acc: 0.6371300220489502\n",
      "Train Loss :1.3411656618118286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 91/138 [09:34<05:10,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.0411347821354866, 2: 0.0587101369138828 \t Acc: 0.6026561260223389\n",
      "Train Loss :1.2777667045593262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 101/138 [10:34<03:54,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.18981462717056274, 2: 0.19061798283058118 \t Acc: 0.6381180286407471\n",
      "Train Loss :1.3135603666305542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 111/138 [11:41<03:03,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.22078046202659607, 2: 0.17208573516740083 \t Acc: 0.6529476642608643\n",
      "Train Loss :1.3088409900665283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 121/138 [12:48<01:55,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.19416415691375732, 2: 0.15769619660267026 \t Acc: 0.6407091617584229\n",
      "Train Loss :1.293066382408142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 131/138 [13:56<00:47,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.22470760345458984, 2: 0.19119884249274297 \t Acc: 0.6164350509643555\n",
      "Train Loss :1.2445024251937866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 138/138 [14:43<00:00,  6.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch loss: 1.2726682536843894, \t ACC/DICE :0.31126808861027594/0.07059948146343231 \n",
      "Starting Train epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/138 [00:06<15:51,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10354732722043991, 2: 0.13382479430430586 \t Acc: 0.6079246997833252\n",
      "Train Loss :1.2758607864379883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 11/138 [01:14<14:33,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.10803426802158356, 2: 0.10651072200521255 \t Acc: 0.6053824424743652\n",
      "Train Loss :1.3071496486663818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 21/138 [02:23<13:54,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09102516621351242, 2: 0.14289883698013314 \t Acc: 0.620511531829834\n",
      "Train Loss :1.2201240062713623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 31/138 [03:30<11:44,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.16621410846710205, 2: 0.17526408947714917 \t Acc: 0.6412861347198486\n",
      "Train Loss :1.282662034034729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 41/138 [04:36<10:37,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dice 1: 0.09596481919288635, 2: 0.0988844612834858 \t Acc: 0.625237226486206\n",
      "Train Loss :1.2888915538787842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 43/138 [04:50<10:47,  6.82s/it]"
     ]
    }
   ],
   "source": [
    "# Run learn from scratch (UNCOMMENT TO RUN)\n",
    "\n",
    "t_loss, t_acc, t_history, v_loss, v_acc, v_history = learn_from_scratch(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
